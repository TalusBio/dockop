{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some indices\n",
    "Even the sparse matrices won't fit in memory. So we will have to loop through them when making predictions or sampling random items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of items:\n",
    "indptr = [0]\n",
    "\n",
    "for chunkID in range(12):\n",
    "    scores = np.load(f'../processed_data/D4_all{chunkID}.npy')\n",
    "    indptr.append(indptr[-1] + scores.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.concatenate([np.load(f'../processed_data/D4_all{i}.npy') for i in range(12)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions to handle the slabs\n",
    "\n",
    "For training, these loop through the chunks and extract the indices that have been selected either at random or suggested by the surrogate model. \n",
    "\n",
    "For predicting, these loop through the chunks and perform the `predict_proba` method on each chunk (after removing the training indices), outputting a concatenated numpy array of predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFPs(chunkID, indptr, isTrain):\n",
    "    fp = sparse.load_npz(f'../processed_data/D4_all{chunkID}.npz')\n",
    "    mask = isTrain[indptr[chunkID]:indptr[chunkID+1]]\n",
    "    return fp[mask]\n",
    "\n",
    "def buildTrain(indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('building training matrix')\n",
    "    fps = sparse.vstack([extractFPs(i, indptr, isTrain) for i in range(12)])\n",
    "    return fps\n",
    "\n",
    "def chunkPredictProba(model, indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('predicting probabilities')\n",
    "    probas = []\n",
    "    for chunkID in range(12):\n",
    "        fps = extractFPs(chunkID, indptr, ~isTrain)\n",
    "        proba = model.predict_proba(fps)[:,1]\n",
    "        probas.append(proba)\n",
    "    return np.concatenate(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and RF regressor and Logistic Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=10000, C=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How long to find the 50k - 200k top 0.3% docking scores from one iteration of Logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSetSizes = [5000, 10_000] + [10000*2<<i for i in range(0,8)]\n",
    "desiredNumLigands = [50_000, 100_000, 150_000, 200_000,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building training matrix\n",
      "predicting probabilities\n",
      "1 5000 50000 1828207\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "2 10000 50000 1709895\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "3 20000 50000 1304008\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "4 40000 50000 991396\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "5 80000 50000 735027\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "6 160000 50000 632774\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "7 320000 50000 695824\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "8 640000 50000 933806\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "9 1280000 50000 1512302\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "10 2560000 50000 2751629\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "11 5000 100000 4922599\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "12 10000 100000 3508388\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "13 20000 100000 2924061\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "14 40000 100000 2176083\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "15 80000 100000 1836107\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "16 160000 100000 1480925\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "17 320000 100000 1420874\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "18 640000 100000 1545100\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "19 1280000 100000 2043012\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "20 2560000 100000 3193733\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "21 5000 150000 10431823\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "22 10000 150000 7840494\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "23 20000 150000 5456465\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "24 40000 150000 4518510\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "25 80000 150000 3845341\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "26 160000 150000 3147567\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "27 320000 150000 2676766\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "28 640000 150000 2552225\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "29 1280000 150000 2913451\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "30 2560000 150000 3973290\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "31 5000 200000 15556345\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "32 10000 200000 14036712\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "33 20000 200000 9326435\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "34 40000 200000 7923825\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "35 80000 200000 6559774\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "36 160000 200000 5486333\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "37 320000 200000 4679698\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "38 640000 200000 4259644\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "39 1280000 200000 4437705\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "40 2560000 200000 5303498\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "41 5000 50000 1315594\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "42 10000 50000 981316\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "43 20000 50000 988498\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "44 40000 50000 786993\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "45 80000 50000 625896\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "46 160000 50000 617455\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "47 320000 50000 674928\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "48 640000 50000 919955\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "49 1280000 50000 1513741\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "50 2560000 50000 2752796\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "51 5000 100000 5364188\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "52 10000 100000 4163804\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "53 20000 100000 3509298\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "54 40000 100000 2437639\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "55 80000 100000 1985237\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "56 160000 100000 1565991\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "57 320000 100000 1426455\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "58 640000 100000 1542619\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "59 1280000 100000 2036777\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "60 2560000 100000 3197706\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "61 5000 150000 7072548\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "62 10000 150000 6413482\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "63 20000 150000 5506492\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "64 40000 150000 4675867\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "65 80000 150000 4063144\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "66 160000 150000 3063829\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "67 320000 150000 2661294\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "68 640000 150000 2539543\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "69 1280000 150000 2890163\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "70 2560000 150000 3962208\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "71 5000 200000 19721144\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "72 10000 200000 14043470\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "73 20000 200000 10221872\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "74 40000 200000 8608794\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "75 80000 200000 7000240\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "76 160000 200000 5407723\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "77 320000 200000 4624839\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "78 640000 200000 4307889\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "79 1280000 200000 4388946\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "80 2560000 200000 5268461\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "81 5000 50000 1375438\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "82 10000 50000 1090323\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "83 20000 50000 1113957\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "84 40000 50000 928240\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "85 80000 50000 697023\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "86 160000 50000 619049\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "87 320000 50000 664704\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "88 640000 50000 921331\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "89 1280000 50000 1515269\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "90 2560000 50000 2751580\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "91 5000 100000 5313809\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "92 10000 100000 3437433\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "93 20000 100000 2791869\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "94 40000 100000 2312279\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "95 80000 100000 1839235\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "96 160000 100000 1524966\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "97 320000 100000 1417273\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "98 640000 100000 1538061\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "99 1280000 100000 2033211\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "100 2560000 100000 3192312\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "101 5000 150000 7633048\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "102 10000 150000 7130976\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "103 20000 150000 5561861\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "104 40000 150000 4291690\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "105 80000 150000 3549169\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "106 160000 150000 2986418\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "107 320000 150000 2604876\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "108 640000 150000 2549132\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "109 1280000 150000 2892659\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "110 2560000 150000 3947925\n",
      "building training matrix\n",
      "predicting probabilities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 5000 200000 15568463\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "112 10000 200000 15114089\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "113 20000 200000 11102076\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "114 40000 200000 8429079\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "115 80000 200000 6433748\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "116 160000 200000 5443631\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "117 320000 200000 4685922\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "118 640000 200000 4303896\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "119 1280000 200000 4401979\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "120 2560000 200000 5260605\n"
     ]
    }
   ],
   "source": [
    "#this is the _actual_ observed cutoff at 0.3th percentile.\n",
    "test_cutoff = np.percentile(scores, 0.3)\n",
    "#mask identifying the top hits.\n",
    "topK = scores<test_cutoff\n",
    "\n",
    "\n",
    "#df = pd.DataFrame(columns=['Algorithm', 'Training size', 'Fraction', 'N hits wanted', 'N hits explored'])\n",
    "df = pd.DataFrame(columns=['Algorithm', 'Training size', 'N hits wanted', 'N hits explored'])\n",
    "count=0\n",
    "\n",
    "for i in range(3):\n",
    "    #for percent in np.array([0.1, 0.25, 0.5, 0.75, 0.9]):\n",
    "    for numWanted in desiredNumLigands:\n",
    "\n",
    "        idx = np.arange(scores.shape[0])\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        for size in trainingSetSizes:\n",
    "            #numWanted = int(percent * scores.shape[0] * 0.003)\n",
    "            #print('numWanted:', numWanted, 'percent:', percent)\n",
    "            #split indices into train and test:\n",
    "            train = idx[:size].copy()\n",
    "            test = idx[size:].copy()\n",
    "            train.sort()\n",
    "            test.sort()\n",
    "    \n",
    "            #generate a 'is a training instance' mask. \n",
    "            isTrain = np.zeros(scores.shape[0]).astype(bool)\n",
    "            isTrain[train]=True\n",
    "    \n",
    "            #topK molecules already found in the training set:\n",
    "            numFound = topK[train].sum()\n",
    "            numRequired = numWanted - numFound\n",
    "            \n",
    "            #fit model:\n",
    "            cutoff = np.percentile(scores[isTrain],0.3)\n",
    "            model.fit(buildTrain(indptr, isTrain, 1), scores[isTrain]<cutoff)\n",
    "\n",
    "            #predict (slowest step):\n",
    "            proba = chunkPredictProba(model, indptr, isTrain, 1)\n",
    "        \n",
    "            #rank the probabilities\n",
    "            proba_sorted = (-proba).argsort()\n",
    "            \n",
    "            #sorted the unseen instances by probability (highest prob first):\n",
    "            test = test[proba_sorted]\n",
    "\n",
    "            #topK molecules already found in the training set:\n",
    "            numSampled = np.argmax(np.cumsum(topK[test])>numRequired)\n",
    "            \n",
    "            #df.loc[count] = ['morgan_feat', size, percent, numWanted, numSampled+size]\n",
    "            df.loc[count] = ['morgan_feat', size, numWanted, numSampled+size]\n",
    "            count+=1\n",
    "            print(count, size, numWanted, numSampled+size)\n",
    "            df.to_csv('../processed_data/D4_single_'+str(0.3)+'.csv')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
