{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some indices\n",
    "Even the sparse matrices won't fit in memory. So we will have to loop through them when making predictions or sampling random items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of items:\n",
    "indptr = [0]\n",
    "\n",
    "for chunkID in range(10):\n",
    "    scores = np.load(f'../processed_data/AmpC_all{chunkID}.npy')\n",
    "    indptr.append(indptr[-1] + scores.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.concatenate([np.load(f'../processed_data/AmpC_all{i}.npy') for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions to handle the slabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFPs(chunkID, indptr, isTrain):\n",
    "    fp = sparse.load_npz(f'../processed_data/AmpC_all{chunkID}.npz')\n",
    "    mask = isTrain[indptr[chunkID]:indptr[chunkID+1]]\n",
    "    return fp[mask]\n",
    "\n",
    "def buildTrain(indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('building training matrix')\n",
    "    fps = sparse.vstack([extractFPs(i, indptr, isTrain) for i in range(10)])\n",
    "    return fps\n",
    "\n",
    "def chunkPredictProba(model, indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('predicting probabilities')\n",
    "    probas = []\n",
    "    for chunkID in range(10):\n",
    "        fps = extractFPs(chunkID, indptr, ~isTrain)\n",
    "        proba = model.predict_proba(fps)[:,1]\n",
    "        probas.append(proba)\n",
    "    return np.concatenate(probas)\n",
    "\n",
    "def chunkPredict(model, indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('predicting probabilities')\n",
    "    preds = []\n",
    "    for chunkID in range(10):\n",
    "        fps = extractFPs(chunkID, indptr, ~isTrain)\n",
    "        pred = -1*model.predict(fps) #best scoring will now be on top (like the proba)\n",
    "        preds.append(pred)\n",
    "    return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and RF regressor and Logistic Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "#model = LogisticRegression(max_iter=10000, C=0.1)\n",
    "model = LogisticRegression(max_iter=10000, C=1)\n",
    "#model = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #trainingSetSizes = np.array([0.4,0.2,0.1])/100\n",
    "# trainingSetSizes = np.array([0.1])/100\n",
    "\n",
    "# trainingSetSizes = (trainingSetSizes * scores.shape[0]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topK = (scores.argsort().argsort() < (scores.shape[0]*0.005)) #0.5th percentile.\n",
    "\n",
    "topK = (scores.argsort().argsort() < 50_000) #~0.05th percentile.\n",
    "tot = topK.sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Altair, using three repeats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 192\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "2 18449\n",
      "building training matrix\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-027d504ee4a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m#fit logreg model:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuildTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misTrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0;31m#fit ridge:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m#model.fit(buildTrain(indptr, isTrain, 1), scores[isTrain])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mprefer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'processes'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m   1408\u001b[0m                                \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             path_func(X, y, pos_class=class_, Cs=[C_],\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    755\u001b[0m             iprint = [-1, 50, 1, 100, 101][\n\u001b[1;32m    756\u001b[0m                 np.searchsorted(np.array([0, 1, 2, 3]), verbose)]\n\u001b[0;32m--> 757\u001b[0;31m             opt_res = optimize.minimize(\n\u001b[0m\u001b[1;32m    758\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L-BFGS-B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    615\u001b[0m                                   **options)\n\u001b[1;32m    616\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    618\u001b[0m                                 callback=callback, **options)\n\u001b[1;32m    619\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_loss_and_grad\u001b[0;34m(w, X, y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# Logistic loss is the negative of the log of the logistic function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dockop/lib/python3.8/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mlog_logistic\u001b[0;34m(X, out)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0m_log_logistic_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_1d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "trainingSetSizes=[400_000, 200_000, 100_000]\n",
    "\n",
    "\n",
    "for percentile in [0.55 ]:    \n",
    "    df = pd.DataFrame(columns=['Algorithm', 'Training size', 'N ligands explored', '% top-k found'])\n",
    "    count=0\n",
    "    \n",
    "    for i in range(3):\n",
    "        idx = np.arange(scores.shape[0])\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        for size in trainingSetSizes:\n",
    "            #split indices into train and test:\n",
    "            train = idx[:size].copy()\n",
    "            test = idx[size:].copy()\n",
    "            train.sort()\n",
    "            test.sort()\n",
    "    \n",
    "            #generate a 'is a training instance' mask. \n",
    "            isTrain = np.zeros(scores.shape[0]).astype(bool)\n",
    "            isTrain[train]=True\n",
    "    \n",
    "            #topK molecules already found in the training set:\n",
    "            numFound = topK[train].sum()\n",
    "        \n",
    "            df.loc[count] = ['morgan_feat', size, train.shape[0], numFound/tot]\n",
    "            count+=1\n",
    "            print(count, numFound)\n",
    "    \n",
    "            #estimate the cutoff once, from the initial random sample:\n",
    "            cutoff = np.percentile(scores[train], percentile)\n",
    "            \n",
    "            for i in range(5):\n",
    "\n",
    "                #fit logreg model:\n",
    "                model.fit(buildTrain(indptr, isTrain, 1), scores[isTrain]<cutoff)\n",
    "                #fit ridge:\n",
    "                #model.fit(buildTrain(indptr, isTrain, 1), scores[isTrain])\n",
    "    \n",
    "                #predict (slowest step) for logreg:\n",
    "                proba = chunkPredictProba(model, indptr, isTrain, 1)\n",
    "                #predict (slowest step) for ridge:\n",
    "                #proba = chunkPredict(model, indptr, isTrain, 1)\n",
    "    \n",
    "                #rank the probabilities\n",
    "                proba_sorted = (-proba).argsort()\n",
    "        \n",
    "                #rank the unseen instances:\n",
    "                test = test[proba_sorted]\n",
    "\n",
    "                #now append the next N instances from the rank ordered unseen instances onto the training set:\n",
    "                train = np.concatenate([train, test[:size]])\n",
    "        \n",
    "                #update the isTrain mask:\n",
    "                isTrain[train]=True\n",
    "        \n",
    "                #now remove those training instances from the test set:\n",
    "                test = test[size:]\n",
    "\n",
    "                #keep the train and test idx arrays sorted so they agree with the chunked* methods:\n",
    "                test.sort()\n",
    "                train.sort()\n",
    "        \n",
    "                #topK molecules already found in the training set:\n",
    "                numFound = topK[train].sum()\n",
    "            \n",
    "                df.loc[count] = ['morgan_feat', size, train.shape[0], numFound/tot]\n",
    "                count+=1\n",
    "                print(count, numFound)\n",
    "                #df.to_csv('../processed_data/ampc_reconstruction_'+str(percentile)+'_1_.csv')\n",
    "                \n",
    "    #df.to_csv('../processed_data/ampc_reconstruction_'+str(percentile)+'_1_.csv')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Training size</th>\n",
       "      <th>N ligands explored</th>\n",
       "      <th>% top-k found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.00428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>800000</td>\n",
       "      <td>0.37858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0.67542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>1600000</td>\n",
       "      <td>0.80326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>2000000</td>\n",
       "      <td>0.87552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>2400000</td>\n",
       "      <td>0.91186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>0.00220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.19212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>600000</td>\n",
       "      <td>0.43718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>800000</td>\n",
       "      <td>0.57908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0.68250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0.74802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.00108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>200000</td>\n",
       "      <td>0.08310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>300000</td>\n",
       "      <td>0.25160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.37566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.46276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>600000</td>\n",
       "      <td>0.52894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.00448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>800000</td>\n",
       "      <td>0.35950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0.67294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>1600000</td>\n",
       "      <td>0.80584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>2000000</td>\n",
       "      <td>0.87580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>2400000</td>\n",
       "      <td>0.91240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>0.00230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.19488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>600000</td>\n",
       "      <td>0.43650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>800000</td>\n",
       "      <td>0.55952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0.63046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0.68256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.00134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>200000</td>\n",
       "      <td>0.09520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>300000</td>\n",
       "      <td>0.26992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.37442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.44000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>600000</td>\n",
       "      <td>0.49106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.00408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>800000</td>\n",
       "      <td>0.36040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0.68146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>1600000</td>\n",
       "      <td>0.80258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>2000000</td>\n",
       "      <td>0.87598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>2400000</td>\n",
       "      <td>0.91070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>0.00212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.21054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>600000</td>\n",
       "      <td>0.43184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>800000</td>\n",
       "      <td>0.53936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0.61872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0.68190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.00122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>200000</td>\n",
       "      <td>0.10438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>300000</td>\n",
       "      <td>0.27230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.37532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.45312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>600000</td>\n",
       "      <td>0.50580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0    Algorithm  Training size  N ligands explored  % top-k found\n",
       "0            0  morgan_feat         400000              400000        0.00428\n",
       "1            1  morgan_feat         400000              800000        0.37858\n",
       "2            2  morgan_feat         400000             1200000        0.67542\n",
       "3            3  morgan_feat         400000             1600000        0.80326\n",
       "4            4  morgan_feat         400000             2000000        0.87552\n",
       "5            5  morgan_feat         400000             2400000        0.91186\n",
       "6            6  morgan_feat         200000              200000        0.00220\n",
       "7            7  morgan_feat         200000              400000        0.19212\n",
       "8            8  morgan_feat         200000              600000        0.43718\n",
       "9            9  morgan_feat         200000              800000        0.57908\n",
       "10          10  morgan_feat         200000             1000000        0.68250\n",
       "11          11  morgan_feat         200000             1200000        0.74802\n",
       "12          12  morgan_feat         100000              100000        0.00108\n",
       "13          13  morgan_feat         100000              200000        0.08310\n",
       "14          14  morgan_feat         100000              300000        0.25160\n",
       "15          15  morgan_feat         100000              400000        0.37566\n",
       "16          16  morgan_feat         100000              500000        0.46276\n",
       "17          17  morgan_feat         100000              600000        0.52894\n",
       "18          18  morgan_feat         400000              400000        0.00448\n",
       "19          19  morgan_feat         400000              800000        0.35950\n",
       "20          20  morgan_feat         400000             1200000        0.67294\n",
       "21          21  morgan_feat         400000             1600000        0.80584\n",
       "22          22  morgan_feat         400000             2000000        0.87580\n",
       "23          23  morgan_feat         400000             2400000        0.91240\n",
       "24          24  morgan_feat         200000              200000        0.00230\n",
       "25          25  morgan_feat         200000              400000        0.19488\n",
       "26          26  morgan_feat         200000              600000        0.43650\n",
       "27          27  morgan_feat         200000              800000        0.55952\n",
       "28          28  morgan_feat         200000             1000000        0.63046\n",
       "29          29  morgan_feat         200000             1200000        0.68256\n",
       "30          30  morgan_feat         100000              100000        0.00134\n",
       "31          31  morgan_feat         100000              200000        0.09520\n",
       "32          32  morgan_feat         100000              300000        0.26992\n",
       "33          33  morgan_feat         100000              400000        0.37442\n",
       "34          34  morgan_feat         100000              500000        0.44000\n",
       "35          35  morgan_feat         100000              600000        0.49106\n",
       "36          36  morgan_feat         400000              400000        0.00408\n",
       "37          37  morgan_feat         400000              800000        0.36040\n",
       "38          38  morgan_feat         400000             1200000        0.68146\n",
       "39          39  morgan_feat         400000             1600000        0.80258\n",
       "40          40  morgan_feat         400000             2000000        0.87598\n",
       "41          41  morgan_feat         400000             2400000        0.91070\n",
       "42          42  morgan_feat         200000              200000        0.00212\n",
       "43          43  morgan_feat         200000              400000        0.21054\n",
       "44          44  morgan_feat         200000              600000        0.43184\n",
       "45          45  morgan_feat         200000              800000        0.53936\n",
       "46          46  morgan_feat         200000             1000000        0.61872\n",
       "47          47  morgan_feat         200000             1200000        0.68190\n",
       "48          48  morgan_feat         100000              100000        0.00122\n",
       "49          49  morgan_feat         100000              200000        0.10438\n",
       "50          50  morgan_feat         100000              300000        0.27230\n",
       "51          51  morgan_feat         100000              400000        0.37532\n",
       "52          52  morgan_feat         100000              500000        0.45312\n",
       "53          53  morgan_feat         100000              600000        0.50580"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../processed_data/ampc_reconstruction_0.15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_results = [['RF (Coley)', 400_000, 71.4, 2.1], ['NN (Coley)', 400_000, 74.7, 1.4],\n",
    "                ['MPN (Coley)',400_000, 87.9, 2.3],\n",
    "    ['RF (Coley)', 200_000, 45.5, 1.8],\n",
    "['NN (Coley)', 200_000, 52.8, 0.5],\n",
    "['MPN (Coley)', 200_000, 67.1, 2.1],\n",
    "['RF (Coley)', 100_000, 24.0, 2.2],\n",
    "['NN (Coley)', 100_000 , 33.3,0.3],\n",
    "['MPN (Coley)', 100_000, 52.0, 0.5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "coley = pd.DataFrame(columns=['Algorithm', 'Training size', 'N ligands explored', '% top-k found'])\n",
    "count = 0 \n",
    "for res in prev_results:\n",
    "    \n",
    "    desired_std_dev = res[3]\n",
    "    samples = np.array([-1,0,1]).astype(float)\n",
    "    samples *= (desired_std_dev/np.std(samples))\n",
    "    for s in samples:\n",
    "        coley.loc[count]= [res[0], res[1], res[1]*6, (s+res[2])/100]\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = pd.concat([df, coley])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-c04776cd748e42dea791fc49d9f7318b\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-c04776cd748e42dea791fc49d9f7318b\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-c04776cd748e42dea791fc49d9f7318b\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-df381babd249e5349b8671d106893dda\"}, \"facet\": {\"column\": {\"type\": \"nominal\", \"field\": \"Training size\", \"sort\": [0.004, 0.002, 0.001]}}, \"spec\": {\"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"ci\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\", \"title\": \"Number of ligands sampled\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"% top-k found\", \"title\": \"% top 50,000 found\"}}}, {\"mark\": {\"type\": \"point\", \"color\": \"black\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"title\": \"% top 50,000 found\"}}}, {\"mark\": {\"type\": \"line\", \"color\": \"black\", \"opacity\": 0.5, \"size\": 1}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"title\": \"% top 50,000 found\"}}}], \"height\": 400, \"width\": 200}, \"resolve\": {\"scale\": {\"x\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-df381babd249e5349b8671d106893dda\": [{\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 400000, \"% top-k found\": 0.00404}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 800000, \"% top-k found\": 0.38906}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 1200000, \"% top-k found\": 0.67318}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 1600000, \"% top-k found\": 0.80184}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 2000000, \"% top-k found\": 0.86378}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.90344}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 200000, \"% top-k found\": 0.00206}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 400000, \"% top-k found\": 0.18784}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 600000, \"% top-k found\": 0.4379}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 800000, \"% top-k found\": 0.58252}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 1000000, \"% top-k found\": 0.666}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.73354}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 100000, \"% top-k found\": 0.00112}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 200000, \"% top-k found\": 0.09054}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 300000, \"% top-k found\": 0.25362}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 400000, \"% top-k found\": 0.36932}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 500000, \"% top-k found\": 0.44916}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.51418}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 400000, \"% top-k found\": 0.00368}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 800000, \"% top-k found\": 0.3594}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 1200000, \"% top-k found\": 0.66538}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 1600000, \"% top-k found\": 0.80418}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 2000000, \"% top-k found\": 0.87196}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.90924}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 200000, \"% top-k found\": 0.00186}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 400000, \"% top-k found\": 0.17938}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 600000, \"% top-k found\": 0.41478}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 800000, \"% top-k found\": 0.56146}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 1000000, \"% top-k found\": 0.64454}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.71214}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 100000, \"% top-k found\": 0.00084}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 200000, \"% top-k found\": 0.08636}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 300000, \"% top-k found\": 0.2489}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 400000, \"% top-k found\": 0.36228}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 500000, \"% top-k found\": 0.45366}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.52164}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 400000, \"% top-k found\": 0.00416}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 800000, \"% top-k found\": 0.39042}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 1200000, \"% top-k found\": 0.67608}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 1600000, \"% top-k found\": 0.79014}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 2000000, \"% top-k found\": 0.86352}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.90486}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 200000, \"% top-k found\": 0.00216}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 400000, \"% top-k found\": 0.19726}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 600000, \"% top-k found\": 0.44508}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 800000, \"% top-k found\": 0.5913}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 1000000, \"% top-k found\": 0.6928}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.75156}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 100000, \"% top-k found\": 0.001}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 200000, \"% top-k found\": 0.09682}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 300000, \"% top-k found\": 0.25202}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 400000, \"% top-k found\": 0.36632}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 500000, \"% top-k found\": 0.44456}, {\"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.51118}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.6882803577007767}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7140000000000001}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7397196422992235}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7298535718005178}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.747}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7641464281994823}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.8508308679579935}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.879}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.9071691320420067}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.43295459231495137}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.455}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.4770454076850486}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.521876275643042}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.528}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.534123724356958}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.6452803577007765}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.6709999999999999}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.6967196422992233}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.21305561282938507}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.24}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.26694438717061497}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.32932576538582525}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.33299999999999996}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.3366742346141747}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.5138762756430421}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.52}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.526123724356958}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_bars = alt.Chart(concat).mark_errorbar(extent='ci').encode(\n",
    "  x=alt.X('N ligands explored:Q',title='Number of ligands sampled'),\n",
    "  y=alt.Y('% top-k found:Q', title='% top 50,000 found'),\n",
    "    color=alt.Color('Algorithm')\n",
    ")\n",
    "\n",
    "points = alt.Chart(concat).mark_point(filled=True, color='black').encode(\n",
    "  x=alt.X('N ligands explored:Q'),\n",
    "  y=alt.Y('% top-k found:Q',aggregate='mean',title='% top 50,000 found'),\n",
    "    color=alt.Color('Algorithm')\n",
    ")\n",
    "\n",
    "line = alt.Chart(concat).mark_line(color='black',size=1,opacity=0.5).encode(\n",
    "  x=alt.X('N ligands explored:Q'),\n",
    "  y=alt.Y('% top-k found:Q',aggregate='mean',title='% top 50,000 found'),\n",
    "    color=alt.Color('Algorithm')\n",
    ")\n",
    "\n",
    "ch = (error_bars+points+line).properties(height=400,width=200).facet(\n",
    "    column=alt.Column('Training size:N',sort=alt.Sort([0.004, 0.002, 0.001])),\n",
    ").resolve_scale(x='independent')\n",
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chs = []\n",
    "for frac in [400000, 200000, 100000]:\n",
    "    \n",
    "    df_ = concat[concat['Training size']==frac].replace('morgan_feat', 'Morgan pharm. & Log.reg. (ours)')\n",
    "    error_bars = alt.Chart(df_).mark_errorbar(extent='ci').encode(\n",
    "          x=alt.X('N ligands explored:Q', \n",
    "                  title='Number of ligands sampled',\n",
    "                  scale=alt.Scale(domain=[0,max(df_['N ligands explored'])+10000])),\n",
    "          y=alt.Y('% top-k found:Q',scale=alt.Scale(domain=[0,0.95])),\n",
    "        color=alt.Color('Algorithm')\n",
    "        )\n",
    "\n",
    "    points = alt.Chart(df_).mark_point(filled=True, color='black').encode(\n",
    "          x=alt.X('N ligands explored:Q'),\n",
    "          y=alt.Y('% top-k found:Q',aggregate='mean',scale=alt.Scale(domain=[0,0.95])),\n",
    "        color=alt.Color('Algorithm')\n",
    "        )\n",
    "\n",
    "    line = alt.Chart(df_).mark_line(color='black',size=1,opacity=0.5).encode(\n",
    "          x=alt.X('N ligands explored:Q'),\n",
    "          y=alt.Y('% top-k found:Q',aggregate='mean',scale=alt.Scale(domain=[0,0.95])),\n",
    "        color=alt.Color('Algorithm')\n",
    "        )\n",
    "    ch = (error_bars+points+line).properties(width=200)\n",
    "    ch.title = str(frac / (100*1e6)*100)\n",
    "    chs.append(ch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chs[0]\n",
    "sup = chs[0] |  chs[1] | chs[2]\n",
    "#sup.save('../figures/ampC_reconstruction.html') #using 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-9397d9b69609479895b890f0b4264f99\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-9397d9b69609479895b890f0b4264f99\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-9397d9b69609479895b890f0b4264f99\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"ci\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\", \"scale\": {\"domain\": [0, 2410000]}, \"title\": \"Number of ligands sampled\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"mark\": {\"type\": \"point\", \"color\": \"black\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"mark\": {\"type\": \"line\", \"color\": \"black\", \"opacity\": 0.5, \"size\": 1}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"ci\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\", \"scale\": {\"domain\": [0, 1210000]}, \"title\": \"Number of ligands sampled\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"mark\": {\"type\": \"point\", \"color\": \"black\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"mark\": {\"type\": \"line\", \"color\": \"black\", \"opacity\": 0.5, \"size\": 1}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}], \"data\": {\"name\": \"data-4521eafb5c925092548119d9c675ed8f\"}, \"title\": \"0.2\", \"width\": 200}, {\"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"ci\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\", \"scale\": {\"domain\": [0, 610000]}, \"title\": \"Number of ligands sampled\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"mark\": {\"type\": \"point\", \"color\": \"black\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"mark\": {\"type\": \"line\", \"color\": \"black\", \"opacity\": 0.5, \"size\": 1}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}], \"data\": {\"name\": \"data-b2d22ce798307d4455916f8ed78d559b\"}, \"title\": \"0.1\", \"width\": 200}], \"data\": {\"name\": \"data-459a33200d9457b7dc8862aa09461853\"}, \"title\": \"0.4\", \"width\": 200, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-459a33200d9457b7dc8862aa09461853\": [{\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 400000, \"% top-k found\": 0.00404}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 800000, \"% top-k found\": 0.38906}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 1200000, \"% top-k found\": 0.67318}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 1600000, \"% top-k found\": 0.80184}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 2000000, \"% top-k found\": 0.86378}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.90344}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 400000, \"% top-k found\": 0.00368}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 800000, \"% top-k found\": 0.3594}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 1200000, \"% top-k found\": 0.66538}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 1600000, \"% top-k found\": 0.80418}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 2000000, \"% top-k found\": 0.87196}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.90924}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 400000, \"% top-k found\": 0.00416}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 800000, \"% top-k found\": 0.39042}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 1200000, \"% top-k found\": 0.67608}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 1600000, \"% top-k found\": 0.79014}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 2000000, \"% top-k found\": 0.86352}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.90486}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.6882803577007767}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7140000000000001}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7397196422992235}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7298535718005178}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.747}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7641464281994823}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.8508308679579935}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.879}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.9071691320420067}], \"data-4521eafb5c925092548119d9c675ed8f\": [{\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 200000, \"% top-k found\": 0.00206}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 400000, \"% top-k found\": 0.18784}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 600000, \"% top-k found\": 0.4379}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 800000, \"% top-k found\": 0.58252}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 1000000, \"% top-k found\": 0.666}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.73354}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 200000, \"% top-k found\": 0.00186}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 400000, \"% top-k found\": 0.17938}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 600000, \"% top-k found\": 0.41478}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 800000, \"% top-k found\": 0.56146}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 1000000, \"% top-k found\": 0.64454}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.71214}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 200000, \"% top-k found\": 0.00216}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 400000, \"% top-k found\": 0.19726}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 600000, \"% top-k found\": 0.44508}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 800000, \"% top-k found\": 0.5913}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 1000000, \"% top-k found\": 0.6928}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.75156}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.43295459231495137}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.455}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.4770454076850486}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.521876275643042}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.528}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.534123724356958}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.6452803577007765}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.6709999999999999}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.6967196422992233}], \"data-b2d22ce798307d4455916f8ed78d559b\": [{\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 100000, \"% top-k found\": 0.00112}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 200000, \"% top-k found\": 0.09054}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 300000, \"% top-k found\": 0.25362}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 400000, \"% top-k found\": 0.36932}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 500000, \"% top-k found\": 0.44916}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.51418}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 100000, \"% top-k found\": 0.00084}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 200000, \"% top-k found\": 0.08636}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 300000, \"% top-k found\": 0.2489}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 400000, \"% top-k found\": 0.36228}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 500000, \"% top-k found\": 0.45366}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.52164}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 100000, \"% top-k found\": 0.001}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 200000, \"% top-k found\": 0.09682}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 300000, \"% top-k found\": 0.25202}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 400000, \"% top-k found\": 0.36632}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 500000, \"% top-k found\": 0.44456}, {\"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.51118}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.21305561282938507}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.24}, {\"Algorithm\": \"RF (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.26694438717061497}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.32932576538582525}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.33299999999999996}, {\"Algorithm\": \"NN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.3366742346141747}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.5138762756430421}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.52}, {\"Algorithm\": \"MPN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.526123724356958}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chs[0]+chs[1]+chs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup.save('../figures/ampC_reconstruction.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
