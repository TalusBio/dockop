{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some indices\n",
    "Even the sparse matrices won't fit in memory. So we will have to loop through them when making predictions or sampling random items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of items:\n",
    "indptr = [0]\n",
    "\n",
    "for chunkID in range(10):\n",
    "    scores = np.load(f'../processed_data/AmpC_all{chunkID}.npy')\n",
    "    indptr.append(indptr[-1] + scores.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.concatenate([np.load(f'../processed_data/AmpC_all{i}.npy') for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions to handle the slabs\n",
    "\n",
    "For training, these loop through the chunks and extract the indices that have been selected either at random or suggested by the surrogate model. \n",
    "\n",
    "For predicting, these loop through the chunks and perform the `predict_proba` method on each chunk (after removing the training indices), outputting a concatenated numpy array of predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFPs(chunkID, indptr, isTrain):\n",
    "    fp = sparse.load_npz(f'../processed_data/AmpC_all{chunkID}.npz')\n",
    "    mask = isTrain[indptr[chunkID]:indptr[chunkID+1]]\n",
    "    return fp[mask]\n",
    "\n",
    "def buildTrain(indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('building training matrix')\n",
    "    fps = sparse.vstack([extractFPs(i, indptr, isTrain) for i in range(10)])\n",
    "    return fps\n",
    "\n",
    "def chunkPredictProba(model, indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('predicting probabilities')\n",
    "    probas = []\n",
    "    for chunkID in range(10):\n",
    "        fps = extractFPs(chunkID, indptr, ~isTrain)\n",
    "        proba = model.predict_proba(fps)[:,1]\n",
    "        probas.append(proba)\n",
    "    return np.concatenate(probas)\n",
    "\n",
    "def chunkPredict(model, indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('predicting probabilities')\n",
    "    preds = []\n",
    "    for chunkID in range(10):\n",
    "        fps = extractFPs(chunkID, indptr, ~isTrain)\n",
    "        pred = -1*model.predict(fps) #best scoring will now be on top (like the proba)\n",
    "        preds.append(pred)\n",
    "    return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Logistic Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, C=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topK = (scores.argsort().argsort() < 50_000) #~0.05th percentile.\n",
    "tot = topK.sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Altair, using three repeats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "trainingSetSizes=[400_000, 200_000, 100_000]\n",
    "\n",
    "\n",
    "for percentile in [0.3 ]:    \n",
    "    df = pd.DataFrame(columns=['Algorithm', 'Training size', 'N ligands explored', '% top-k found'])\n",
    "    count=0\n",
    "    \n",
    "    for i in range(3):\n",
    "        idx = np.arange(scores.shape[0])\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        for size in trainingSetSizes:\n",
    "            #split indices into train and test:\n",
    "            train = idx[:size].copy()\n",
    "            test = idx[size:].copy()\n",
    "            train.sort()\n",
    "            test.sort()\n",
    "    \n",
    "            #generate a 'is a training instance' mask. \n",
    "            isTrain = np.zeros(scores.shape[0]).astype(bool)\n",
    "            isTrain[train]=True\n",
    "    \n",
    "            #topK molecules already found in the training set:\n",
    "            numFound = topK[train].sum()\n",
    "        \n",
    "            df.loc[count] = ['morgan_feat', size, train.shape[0], numFound/tot]\n",
    "            count+=1\n",
    "            print(count, numFound)\n",
    "    \n",
    "            #estimate the cutoff once, from the initial random sample:\n",
    "            cutoff = np.percentile(scores[train], percentile)\n",
    "            \n",
    "            for i in range(5):\n",
    "\n",
    "                #fit logreg model:\n",
    "                model.fit(buildTrain(indptr, isTrain, 1), scores[isTrain]<cutoff)\n",
    "                #fit ridge:\n",
    "                #model.fit(buildTrain(indptr, isTrain, 1), scores[isTrain])\n",
    "    \n",
    "                #predict (slowest step) for logreg:\n",
    "                proba = chunkPredictProba(model, indptr, isTrain, 1)\n",
    "                #predict (slowest step) for ridge:\n",
    "                #proba = chunkPredict(model, indptr, isTrain, 1)\n",
    "    \n",
    "                #rank the probabilities\n",
    "                proba_sorted = (-proba).argsort()\n",
    "        \n",
    "                #rank the unseen instances:\n",
    "                test = test[proba_sorted]\n",
    "\n",
    "                #now append the next N instances from the rank ordered unseen instances onto the training set:\n",
    "                train = np.concatenate([train, test[:size]])\n",
    "        \n",
    "                #update the isTrain mask:\n",
    "                isTrain[train]=True\n",
    "        \n",
    "                #now remove those training instances from the test set:\n",
    "                test = test[size:]\n",
    "\n",
    "                #keep the train and test idx arrays sorted so they agree with the chunked* methods:\n",
    "                test.sort()\n",
    "                train.sort()\n",
    "        \n",
    "                #topK molecules already found in the training set:\n",
    "                numFound = topK[train].sum()\n",
    "            \n",
    "                df.loc[count] = ['morgan_feat', size, train.shape[0], numFound/tot]\n",
    "                count+=1\n",
    "                print(count, numFound)\n",
    "                df.to_csv('../processed_data/ampc_reconstruction_'+str(percentile)+'_1_.csv')\n",
    "                \n",
    "    df.to_csv('../processed_data/ampc_reconstruction_'+str(percentile)+'_1_.csv')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results look like this:\n",
    "And they can be plotted using `./plot_scripts/plot_wholedataset.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv('../processed_data/ampc_reconstruction_0.3_1_.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
