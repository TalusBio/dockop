{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import tqdm\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from functools import lru_cache\n",
    "from modAL.models import ActiveLearner\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from functools import partial\n",
    "from modAL.batch import uncertainty_batch_sampling\n",
    "from modAL.models import BayesianOptimizer\n",
    "from modAL.acquisition import optimizer_EI, max_EI\n",
    "from sklearn.metrics import recall_score\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('default')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.renderers.enable('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_CHUNKS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some indices\n",
    "Even the sparse matrices won't fit in memory. So we will have to loop through them when making predictions or sampling random items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_EMBEDDINGS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECEPTOR = \"EnamineHTS\"\n",
    "DATA_DIR = \"/mnt/efs/enamine\"\n",
    "\n",
    "if USE_EMBEDDINGS:\n",
    "    OUTPUT_RESULTS_FILE = f\"{RECEPTOR}_embedding_results.csv\"\n",
    "else:\n",
    "    OUTPUT_RESULTS_FILE = f\"{RECEPTOR}_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of items:\n",
    "indptr = [0]\n",
    "scores_lst = []\n",
    "\n",
    "for chunk_id in range(NUM_CHUNKS):\n",
    "    scores = np.load(f\"{DATA_DIR}/{RECEPTOR}_scores_{chunk_id}.npy\")\n",
    "    indptr.append(indptr[-1] + scores.shape[0])\n",
    "    scores_lst.append(scores)\n",
    "    \n",
    "scores = np.concatenate(scores_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2104318,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(chunk_id, use_embeddings=True):\n",
    "    print(\"Loading vectors\", end=\"; \", flush=True)\n",
    "    if use_embeddings:\n",
    "        vectors = np.load(f\"{DATA_DIR}/{RECEPTOR}_embeddings_{chunk_id}.npy\")\n",
    "    else:\n",
    "        vectors = sparse.load_npz(f\"{DATA_DIR}/{RECEPTOR}_fingerprints_{chunk_id}.npz\")\n",
    "    return vectors\n",
    "\n",
    "def extract_vectors(chunk_id, indptr, is_train):\n",
    "    print(f\"Extracting vectors: {chunk_id}\", end=\"; \", flush=True)\n",
    "    vectors = load_vectors(chunk_id, use_embeddings=USE_EMBEDDINGS)\n",
    "    mask = is_train[indptr[chunk_id]:indptr[chunk_id+1]]\n",
    "    return vectors[mask].astype(int)\n",
    "\n",
    "def build_train(indptr, is_train):\n",
    "    print(\"Building training set\", end=\"; \", flush=True)\n",
    "    if USE_EMBEDDINGS:\n",
    "        vectors = np.vstack([extract_vectors(i, tuple(indptr), is_train) for i in range(NUM_CHUNKS)])\n",
    "    else:\n",
    "        vectors = sparse.vstack([extract_vectors(i, tuple(indptr), is_train) for i in range(NUM_CHUNKS)])  \n",
    "    return vectors\n",
    "\n",
    "def chunk_predict_proba(model, indptr, is_train):\n",
    "    print(\"Predicting proba\", end=\"; \", flush=True)\n",
    "    probas = []\n",
    "    for chunk_id in range(NUM_CHUNKS):\n",
    "        vectors = extract_vectors(chunk_id, indptr, ~is_train)\n",
    "        proba = model.predict_proba(vectors)[:,1]\n",
    "        probas.append(proba)\n",
    "    return np.concatenate(probas)\n",
    "\n",
    "def chunk_predict(model, indptr, is_train):\n",
    "    print(\"Predicting scores\", end=\"; \", flush=True)\n",
    "    preds = []\n",
    "    for chunk_id in range(NUM_CHUNKS):\n",
    "        vectors = extract_vectors(chunk_id, indptr, ~is_train)\n",
    "        pred = model.predict(vectors)\n",
    "        preds.append(pred)\n",
    "    return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Logistic Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = LogisticRegression(max_iter=10000, C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBRegressor(\n",
    "# #     objective=\"reg:squaredlogerror\"\n",
    "#     use_label_encoder=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(Y_mean: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Greedy acquisition score\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_mean : np.ndarray\n",
    "        the mean predicted y values\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        the greedy acquisition scores\n",
    "    \"\"\"\n",
    "    return Y_mean\n",
    "\n",
    "def ucb(Y_mean: np.ndarray, Y_var: np.ndarray, beta: int = 2) -> float:\n",
    "    \"\"\"Upper confidence bound acquisition score\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_mean : np.ndarray\n",
    "    Y_var : np.ndarray\n",
    "        the variance of the mean predicted y values\n",
    "    beta : int (Default = 2)\n",
    "        the number of standard deviations to add to Y_mean\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        the upper confidence bound acquisition scores\n",
    "    \"\"\"\n",
    "    return Y_mean + beta*np.sqrt(Y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_means_and_vars(model, x):\n",
    "    preds = np.zeros((len(x), len(model.estimators_)))\n",
    "\n",
    "    for j, submodel in enumerate(model.estimators_):\n",
    "        preds[:, j] = submodel.predict(x)\n",
    "\n",
    "    return np.mean(preds, axis=1), np.var(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    min_samples_leaf=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors; "
     ]
    }
   ],
   "source": [
    "x_raw = load_vectors(chunk_id=0, use_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K_THRESHOLD = 1_000\n",
    "N_QUERIES = 5\n",
    "N_FOLDS = 3\n",
    "FRACTION = 0.004\n",
    "# PERCENTILE = 0.3\n",
    "BATCH_SIZE = int(len(scores)*FRACTION)\n",
    "preset_batch = partial(uncertainty_batch_sampling, n_instances=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labeled_examples = scores.shape[0]\n",
    "train_indices = np.array(random.sample(range(n_labeled_examples+1), k=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(scores.shape[0]).astype(bool)\n",
    "mask[train_indices] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutoff = np.percentile(scores[train_indices], PERCENTILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_raw\n",
    "y_test = (scores.argsort().argsort() < TOP_K_THRESHOLD)\n",
    "y_raw = scores\n",
    "# y_raw = scores < cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_raw[mask]\n",
    "y_train = y_raw[mask]\n",
    "x_pool = x_raw[~mask]\n",
    "y_pool = y_raw[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-34322bd80f9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Iteration: -1, Recall: {recall}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dockop/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dockop/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mrecall_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1772\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \"\"\"\n\u001b[0;32m-> 1774\u001b[0;31m     _, r, _, _ = precision_recall_fscore_support(y_true, y_pred,\n\u001b[0m\u001b[1;32m   1775\u001b[0m                                                  \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m                                                  \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dockop/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dockop/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1464\u001b[0;31m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0m\u001b[1;32m   1465\u001b[0m                                     pos_label)\n\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dockop/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1275\u001b[0m                          str(average_options))\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m     \u001b[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dockop/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0m\u001b[1;32m     93\u001b[0m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "instances_seen = BATCH_SIZE\n",
    "\n",
    "y_pred = learner.predict(X=x_test)\n",
    "recall = recall_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "print(f\"Iteration: -1, Recall: {recall}\")\n",
    "results.append({\n",
    "    \"Training size\": BATCH_SIZE, \n",
    "    \"N ligands explore\": instances_seen,\n",
    "    \"% top-k found\": recall\n",
    "})\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for index in range(N_QUERIES-1):\n",
    "    fn_start = time.time()\n",
    "    print(\"Querying instances...\")\n",
    "    query_index, query_instance = learner.query(x_pool)\n",
    "    print(f\"Took {(time.time() - fn_start)/60} minutes\")\n",
    "    \n",
    "    fn_start = time.time()\n",
    "    print(\"Teaching...\")\n",
    "    # Teach our ActiveLearner model the record it has requested.\n",
    "    x, y = x_pool[query_index], y_pool[query_index]\n",
    "    learner.teach(X=x, y=y)\n",
    "    print(f\"Took {(time.time() - fn_start)/60} minutes\")\n",
    "    \n",
    "    # Remove the queried instance from the unlabeled pool.\n",
    "    pool_mask = np.zeros(x_pool.shape[0]).astype(bool)\n",
    "    pool_mask[query_index] = True\n",
    "    x_pool = x_pool[~pool_mask]\n",
    "    y_pool = y_pool[~pool_mask]\n",
    "    \n",
    "    fn_start = time.time()\n",
    "    print(\"Predicting...\")\n",
    "    y_pred = learner.predict(X=x_test)\n",
    "    recall = recall_score(y_true=y_test, y_pred=y_pred)\n",
    "    instances_seen += BATCH_SIZE\n",
    "    print(f\"Took {(time.time() - fn_start)/60} minutes\")\n",
    "    \n",
    "    print(f\"Iteration: {index}, Recall: {recall}\")\n",
    "    \n",
    "    results.append({\n",
    "        \"Training size\": BATCH_SIZE, \n",
    "        \"N ligands explore\": instances_seen,\n",
    "        \"% top-k found\": recall\n",
    "    })\n",
    "    \n",
    "print(f\"Took {(time.time() - start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training_set_fractions = [0.004, 0.002, 0.001]\n",
    "training_set_fractions = [0.004]\n",
    "\n",
    "percentile = 0.3\n",
    "\n",
    "df = pd.DataFrame(columns=['Algorithm', 'Training size', 'N ligands explored', '% top-k found'])\n",
    "count = 0\n",
    "\n",
    "for i in range(3):\n",
    "    idx = np.arange()\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    for fraction in training_set_fractions:\n",
    "        size = int(len(scores) * fraction)\n",
    "        \n",
    "        # split indices into train and test:\n",
    "        train_indices = idx[:size].copy()\n",
    "        test_indices = idx[size:].copy()\n",
    "        train_indices.sort()\n",
    "        test_indices.sort()\n",
    "\n",
    "        # generate a 'is a training instance' mask. \n",
    "        is_train = np.zeros(scores.shape[0]).astype(bool)\n",
    "        is_train[train_indices] = True\n",
    "\n",
    "        # top_k molecules already found in the training set:\n",
    "        num_found = top_k[train_indices].sum()\n",
    "\n",
    "        df.loc[count] = [\"morgan_feat\", size, train_indices.shape[0], num_found/total]\n",
    "        count += 1\n",
    "        print(f\"Iteration: {count}, Found {num_found} top k ligands\")\n",
    "\n",
    "        # estimate the cutoff once, from the initial random sample:\n",
    "        cutoff = np.percentile(scores[train_indices], percentile)\n",
    "\n",
    "        for i in range(5):\n",
    "            # fit logreg model:\n",
    "            x_train = build_train(indptr, is_train)\n",
    "            y_train = scores[is_train] < cutoff\n",
    "\n",
    "            print(\"Fitting model\", end=\"; \", flush=True)\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            # predict (slowest step) for logreg:\n",
    "            proba = chunk_predict_proba(model, indptr, is_train)\n",
    "\n",
    "            # rank the probabilities\n",
    "            proba_sorted = (-proba).argsort()\n",
    "\n",
    "            # rank the unseen instances:\n",
    "            test_indices = test_indices[proba_sorted]\n",
    "\n",
    "            # now append the next N instances from the rank ordered unseen instances onto the training set:\n",
    "            train_indices = np.concatenate([train_indices, test_indices[:size]])\n",
    "\n",
    "            # update the isTrain mask and remove those training instances from the test set\n",
    "            is_train[train_indices] = True\n",
    "            test_indices = test_indices[size:]\n",
    "\n",
    "            # keep the train and test idx arrays sorted so they agree with the chunked* methods:\n",
    "            test_indices.sort()\n",
    "            train_indices.sort()\n",
    "\n",
    "            # topK molecules already found in the training set:\n",
    "            num_found = top_k[train_indices].sum()\n",
    "\n",
    "            df.loc[count] = ['morgan_feat', size, train_indices.shape[0], num_found/total]\n",
    "            count += 1\n",
    "            \n",
    "            print(f\"\\nIteration: {count}, Found {num_found} top k ligands\")\n",
    "            \n",
    "            df.to_csv(f\"{DATA_DIR}/{OUTPUT_RESULTS_FILE}\")\n",
    "\n",
    "df.to_csv(f\"{DATA_DIR}/{OUTPUT_RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results look like this:\n",
    "And they can be plotted using `./plot_scripts/plot_wholedataset.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(f\"{DATA_DIR}/{RECEPTOR}_embedding_results.csv\", index_col=0)\n",
    "df1['Algorithm'] = 'LogReg (embeddings)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.read_csv(f\"{DATA_DIR}/{RECEPTOR}_results.csv\", index_col=0)\n",
    "# df2['Algorithm'] = 'LogReg (fps)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_results = [['RF (Graff)', 8_417, 84.3, 1.1], \n",
    "                ['NN (Graff)', 8_417, 95.7, 0.1],\n",
    "                ['MPN (Graff)',8_417, 97.6, 0.3],\n",
    "                ['random', 8_417, 2.6, 0.1],\n",
    "                ['RF (Graff)', 4_208, 72.3, 1.9],\n",
    "                ['NN (Graff)', 4_208, 88.8, 0.8],\n",
    "                ['MPN (Graff)', 4_208, 93.3, 0.9],\n",
    "                ['random', 4_208, 1.3, 0.4],\n",
    "                ['RF (Graff)', 2_104, 55.8, 4.9],\n",
    "                ['NN (Graff)', 2_104 , 70.5, 1.8],\n",
    "                ['MPN (Graff)', 2_104, 78.5, 2.2],\n",
    "                ['random', 2_104, 0.6, 0.2]]\n",
    "\n",
    "coley = pd.DataFrame(columns=['Algorithm', 'Training size', 'N ligands explored', '% top-k found'])\n",
    "count = 0 \n",
    "for res in prev_results:\n",
    "    desired_std_dev = res[3]\n",
    "    samples = np.array([-1,0,1]).astype(float)\n",
    "    samples *= (desired_std_dev/np.std(samples))\n",
    "    for s in samples:\n",
    "        coley.loc[count]= [res[0], res[1], res[1]*6, (s+res[2])/100]\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = pd.concat([df, coley])\n",
    "concat['% top-k found'] *= 100\n",
    "concat.columns = ['Algorithm', 'Training set size', 'N ligands explored', '% top-k found']\n",
    "concat['Training set size'] = concat['Training set size'].apply(lambda num: f\"{num:,d}\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_bars = alt.Chart(concat).mark_errorbar(extent='ci').encode(\n",
    "  x=alt.X('N ligands explored:Q',title='Number of ligands sampled'),\n",
    "  y=alt.Y('% top-k found:Q', title='% top 1,000 found'),\n",
    "    color=alt.Color('Algorithm')\n",
    ")\n",
    "\n",
    "points = alt.Chart(concat).mark_point(filled=False, size=40, color='black').encode(\n",
    "  x=alt.X('N ligands explored:Q'),\n",
    "  y=alt.Y('% top-k found:Q',aggregate='mean',title='% top 1,000 found'),\n",
    "    color=alt.Color('Algorithm'),\n",
    "    tooltip=alt.Tooltip('% top-k found:Q',aggregate='mean',title='% top 1,000 found')\n",
    ")\n",
    "\n",
    "line = alt.Chart(concat).mark_line(color='black',size=2,opacity=0.5).encode(\n",
    "  x=alt.X('N ligands explored:Q'),\n",
    "  y=alt.Y('% top-k found:Q',aggregate='mean',title='% top 1,000 found'),\n",
    "    color=alt.Color('Algorithm')\n",
    ")\n",
    "\n",
    "ch = (error_bars+points+line).properties(height=300,width=150).facet(\n",
    "    column=alt.Column('Training set size:N',sort=alt.Sort([0.004, 0.002, 0.001])),\n",
    ").resolve_scale(x='independent')\n",
    "# ch.save('../../figures/active_learning_percentage.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = load_vectors(chunk_id=0, use_embeddings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = scores < cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2, random_state=42)\n",
    "pca = TruncatedSVD(n_components=2, random_state=42)\n",
    "transformed_vectors = pca.fit_transform(X=vectors)\n",
    "\n",
    "# Isolate the data we'll need for plotting.\n",
    "x_component, y_component = transformed_vectors[:, 0], transformed_vectors[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(x_component, y_component, classes):\n",
    "    # Plot our dimensionality-reduced (via PCA) dataset.\n",
    "    plt.figure(figsize=(8.5, 6), dpi=130)\n",
    "    plt.scatter(x=x_component, y=y_component, c=classes, s=5, alpha=0.5)\n",
    "    plt.title('Ligands after PCA transformation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(x_component=x_component, y_component=y_component, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dockop",
   "language": "python",
   "name": "dockop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
