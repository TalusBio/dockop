{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some indices\n",
    "Even the sparse matrices won't fit in memory. So we will have to loop through them when making predictions or sampling random items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of items:\n",
    "indptr = [0]\n",
    "\n",
    "for chunkID in range(12):\n",
    "    scores = np.load(f'../processed_data/D4_all{chunkID}.npy')\n",
    "    indptr.append(indptr[-1] + scores.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.concatenate([np.load(f'../processed_data/D4_all{i}.npy') for i in range(12)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions to handle the slabs\n",
    "\n",
    "For training, these loop through the chunks and extract the indices that have been selected either at random or suggested by the surrogate model. \n",
    "\n",
    "For predicting, these loop through the chunks and perform the `predict_proba` method on each chunk (after removing the training indices), outputting a concatenated numpy array of predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFPs(chunkID, indptr, isTrain):\n",
    "    fp = sparse.load_npz(f'../processed_data/D4_all{chunkID}.npz')\n",
    "    mask = isTrain[indptr[chunkID]:indptr[chunkID+1]]\n",
    "    return fp[mask]\n",
    "\n",
    "def buildTrain(indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('building training matrix')\n",
    "    fps = sparse.vstack([extractFPs(i, indptr, isTrain) for i in range(12)])\n",
    "    return fps\n",
    "\n",
    "def chunkPredictProba(model, indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('predicting probabilities')\n",
    "    probas = []\n",
    "    for chunkID in range(12):\n",
    "        fps = extractFPs(chunkID, indptr, ~isTrain)\n",
    "        proba = model.predict_proba(fps)[:,1]\n",
    "        probas.append(proba)\n",
    "    return np.concatenate(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and RF regressor and Logistic Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=10000, C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = (scores.argsort().argsort() < (scores.shape[0]*0.0005)) #0.05th percentile.\n",
    "\n",
    "#topK = (scores.argsort().argsort() < 50_000) #~0.05th percentile for AmpC, but not for D4\n",
    "#tot = topK.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tot = topK.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58121\n"
     ]
    }
   ],
   "source": [
    "print(tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Altair, using three repeats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 197\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "2 14990\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "3 24587\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "4 31220\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "5 34899\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "6 37686\n",
      "7 111\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "8 9032\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "9 16379\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "10 22244\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "11 26431\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "12 29556\n",
      "13 47\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "14 4577\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "15 9727\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "16 14522\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "17 18300\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "18 21242\n",
      "19 215\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "20 14618\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "21 24103\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "22 30439\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "23 34619\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "24 37438\n",
      "25 119\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "26 7594\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "27 15713\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "28 21635\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "29 25894\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "30 29267\n",
      "31 59\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "32 3309\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "33 9382\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "34 14184\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "35 17818\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "36 20858\n",
      "37 201\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "38 15286\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "39 25026\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "40 31341\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "41 35175\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "42 37876\n",
      "43 112\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "44 8334\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "45 16813\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "46 23055\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "47 27006\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "48 29911\n",
      "49 55\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "50 3867\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "51 9693\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "52 14881\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "53 18945\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "54 21718\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainingSetSizes=[400_000, 200_000, 100_000]\n",
    "#for percentile in [0.05, 0.1, 0.25, 0.5, 0.75, 1 ]:\n",
    "for percentile in [0.3]:\n",
    "    \n",
    "    df = pd.DataFrame(columns=['Algorithm', 'Training size', 'N ligands explored', '% top-k found'])\n",
    "    count=0\n",
    "    \n",
    "    for i in range(3):\n",
    "        idx = np.arange(scores.shape[0])\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        for size in trainingSetSizes:\n",
    "            #split indices into train and test:\n",
    "            train = idx[:size].copy()\n",
    "            test = idx[size:].copy()\n",
    "            train.sort()\n",
    "            test.sort()\n",
    "    \n",
    "            #generate a 'is a training instance' mask. \n",
    "            isTrain = np.zeros(scores.shape[0]).astype(bool)\n",
    "            isTrain[train]=True\n",
    "    \n",
    "            #topK molecules already found in the training set:\n",
    "            numFound = topK[train].sum()\n",
    "        \n",
    "            df.loc[count] = ['morgan_feat', size, train.shape[0], numFound/tot]\n",
    "            count+=1\n",
    "            print(count, numFound)\n",
    "            \n",
    "            #estimate the cutoff once, from the initial random sample:\n",
    "            cutoff = np.percentile(scores[train], percentile)\n",
    "    \n",
    "            for i in range(5):\n",
    "                \n",
    "                #fit model:\n",
    "                model.fit(buildTrain(indptr, isTrain, 1), scores[isTrain]<cutoff)\n",
    "    \n",
    "                #predict (slowest step):\n",
    "                proba = chunkPredictProba(model, indptr, isTrain, 1)\n",
    "    \n",
    "                #rank the probabilities\n",
    "                proba_sorted = (-proba).argsort()\n",
    "        \n",
    "                #rank the unseen instances:\n",
    "                test = test[proba_sorted]\n",
    "\n",
    "                #now append the next N instances from the rank ordered unseen instances onto the training set:\n",
    "                train = np.concatenate([train, test[:size]])\n",
    "        \n",
    "                #update the isTrain mask:\n",
    "                isTrain[train]=True\n",
    "        \n",
    "                #now remove those training instances from the test set:\n",
    "                test = test[size:]\n",
    "\n",
    "                #keep the train and test idx arrays sorted so they agree with the chunked* methods:\n",
    "                test.sort()\n",
    "                train.sort()\n",
    "        \n",
    "                #topK molecules already found in the training set:\n",
    "                numFound = topK[train].sum()\n",
    "            \n",
    "                df.loc[count] = ['morgan_feat', size, train.shape[0], numFound/tot]\n",
    "                count+=1\n",
    "                print(count, numFound)\n",
    "                df.to_csv('../processed_data/D4_reconstruction_'+str(percentile)+'_1_.csv')\n",
    "                \n",
    "    df.to_csv('../processed_data/D4_reconstruction_'+str(percentile)+'_1_.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
