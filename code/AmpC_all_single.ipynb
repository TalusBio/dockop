{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some indices\n",
    "Even the sparse matrices won't fit in memory. So we will have to loop through them when making predictions or sampling random items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of items:\n",
    "indptr = [0]\n",
    "\n",
    "for chunkID in range(10):\n",
    "    scores = np.load(f'../processed_data/AmpC_all{chunkID}.npy')\n",
    "    indptr.append(indptr[-1] + scores.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.concatenate([np.load(f'../processed_data/AmpC_all{i}.npy') for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions to handle the slabs\n",
    "\n",
    "For training, these loop through the chunks and extract the indices that have been selected either at random or suggested by the surrogate model. \n",
    "\n",
    "For predicting, these loop through the chunks and perform the `predict_proba` method on each chunk (after removing the training indices), outputting a concatenated numpy array of predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFPs(chunkID, indptr, isTrain):\n",
    "    fp = sparse.load_npz(f'../processed_data/AmpC_all{chunkID}.npz')\n",
    "    mask = isTrain[indptr[chunkID]:indptr[chunkID+1]]\n",
    "    return fp[mask]\n",
    "\n",
    "def buildTrain(indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('building training matrix')\n",
    "    fps = sparse.vstack([extractFPs(i, indptr, isTrain) for i in range(10)])\n",
    "    return fps\n",
    "\n",
    "def chunkPredictProba(model, indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('predicting probabilities')\n",
    "    probas = []\n",
    "    for chunkID in range(10):\n",
    "        fps = extractFPs(chunkID, indptr, ~isTrain)\n",
    "        proba = model.predict_proba(fps)[:,1]\n",
    "        probas.append(proba)\n",
    "    return np.concatenate(probas)\n",
    "\n",
    "def chunkPredict(model, indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('predicting probabilities')\n",
    "    preds = []\n",
    "    for chunkID in range(10):\n",
    "        fps = extractFPs(chunkID, indptr, ~isTrain)\n",
    "        pred = -1*model.predict(fps) #best scoring will now be on top (like the proba)\n",
    "        preds.append(pred)\n",
    "    return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and RF regressor and Logistic Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, C=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How long to find the 50k - 200k top 0.3% docking scores from one iteration of Logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSetSizes = [5000, 10_000] + [10000*2<<i for i in range(0,8)]\n",
    "\n",
    "num_actual = scores.shape[0] * 0.003\n",
    "\n",
    "desiredNumLigands = [50_000, 100_000, 150_000, 200_000,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building training matrix\n",
      "predicting probabilities\n",
      "1 5000 50000 344329\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "2 10000 50000 336375\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "3 20000 50000 324524\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "4 40000 50000 310417\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "5 80000 50000 336111\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "6 160000 50000 380939\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "7 320000 50000 506687\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "8 640000 50000 801795\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "9 1280000 50000 1419876\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "10 2560000 50000 2675204\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "11 5000 100000 925661\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "12 10000 100000 884846\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "13 20000 100000 760983\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "14 40000 100000 661794\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "15 80000 100000 674695\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "16 160000 100000 704185\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "17 320000 100000 782183\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "18 640000 100000 1043060\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "19 1280000 100000 1642015\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "20 2560000 100000 2879192\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "21 5000 150000 1550510\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "22 10000 150000 1521830\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "23 20000 150000 1446115\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "24 40000 150000 1280795\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "25 80000 150000 1198564\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "26 160000 150000 1164833\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "27 320000 150000 1221563\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "28 640000 150000 1421516\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "29 1280000 150000 1967853\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "30 2560000 150000 3178783\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "31 5000 200000 2955854\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "32 10000 200000 2541541\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "33 20000 200000 2355500\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "34 40000 200000 1972800\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "35 80000 200000 1848854\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "36 160000 200000 1795694\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "37 320000 200000 1804942\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "38 640000 200000 1957273\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "39 1280000 200000 2466828\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "40 2560000 200000 3632603\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "41 5000 50000 356073\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "42 10000 50000 329394\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "43 20000 50000 338673\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "44 40000 50000 312439\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "45 80000 50000 325205\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "46 160000 50000 371133\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "47 320000 50000 501417\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "48 640000 50000 796357\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "49 1280000 50000 1417731\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "50 2560000 50000 2674410\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "51 5000 100000 963828\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "52 10000 100000 816796\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "53 20000 100000 758859\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "54 40000 100000 734093\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "55 80000 100000 704545\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "56 160000 100000 710873\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "57 320000 100000 795198\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "58 640000 100000 1052938\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "59 1280000 100000 1645322\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "60 2560000 100000 2878395\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "61 5000 150000 1755918\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "62 10000 150000 1744018\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "63 20000 150000 1394320\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "64 40000 150000 1282663\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "65 80000 150000 1144892\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "66 160000 150000 1110777\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "67 320000 150000 1178953\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "68 640000 150000 1401556\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "69 1280000 150000 1963713\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "70 2560000 150000 3174718\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "71 5000 200000 2674839\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "72 10000 200000 2361335\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "73 20000 200000 2229416\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "74 40000 200000 2186024\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "75 80000 200000 1844554\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "76 160000 200000 1744138\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "77 320000 200000 1736563\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "78 640000 200000 1911821\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "79 1280000 200000 2454408\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "80 2560000 200000 3629992\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "81 5000 50000 384781\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "82 10000 50000 410835\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "83 20000 50000 342603\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "84 40000 50000 325796\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "85 80000 50000 329532\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "86 160000 50000 373667\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "87 320000 50000 498938\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "88 640000 50000 800257\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "89 1280000 50000 1414976\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "90 2560000 50000 2674262\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "91 5000 100000 1039155\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "92 10000 100000 807614\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "93 20000 100000 741050\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "94 40000 100000 684578\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "95 80000 100000 684433\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "96 160000 100000 695293\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "97 320000 100000 778416\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "98 640000 100000 1045061\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "99 1280000 100000 1640733\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "100 2560000 100000 2879418\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "101 5000 150000 1539112\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "102 10000 150000 1289590\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "103 20000 150000 1300683\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "104 40000 150000 1160184\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "105 80000 150000 1150591\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "106 160000 150000 1138094\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "107 320000 150000 1183804\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "108 640000 150000 1415235\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "109 1280000 150000 1972052\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "110 2560000 150000 3175958\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "111 5000 200000 2144191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building training matrix\n",
      "predicting probabilities\n",
      "112 10000 200000 2141253\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "113 20000 200000 2160221\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "114 40000 200000 2008074\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "115 80000 200000 1853942\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "116 160000 200000 1762290\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "117 320000 200000 1751264\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "118 640000 200000 1924092\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "119 1280000 200000 2462034\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "120 2560000 200000 3630749\n"
     ]
    }
   ],
   "source": [
    "#this is the _actual_ observed cutoff at 0.3th percentile.\n",
    "test_cutoff = np.percentile(scores, 0.3)\n",
    "#mask identifying the top hits.\n",
    "topK = scores<test_cutoff\n",
    "\n",
    "\n",
    "#df = pd.DataFrame(columns=['Algorithm', 'Training size', 'Fraction', 'N hits wanted', 'N hits explored'])\n",
    "df = pd.DataFrame(columns=['Algorithm', 'Training size',  'N hits wanted', 'N hits explored'])\n",
    "count=0\n",
    "\n",
    "for i in range(3):\n",
    "    #for percent in np.array([0.1, 0.25, 0.5, 0.75, 0.9]):\n",
    "    for numWanted in desiredNumLigands:\n",
    "\n",
    "        idx = np.arange(scores.shape[0])\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        for size in trainingSetSizes:\n",
    "            #numWanted = int(percent * scores.shape[0] * 0.003)\n",
    "            #print('numWanted:', numWanted, 'percent:', percent)\n",
    "            #split indices into train and test:\n",
    "            train = idx[:size].copy()\n",
    "            test = idx[size:].copy()\n",
    "            train.sort()\n",
    "            test.sort()\n",
    "    \n",
    "            #generate a 'is a training instance' mask. \n",
    "            isTrain = np.zeros(scores.shape[0]).astype(bool)\n",
    "            isTrain[train]=True\n",
    "    \n",
    "            #topK molecules already found in the training set:\n",
    "            numFound = topK[train].sum()\n",
    "            numRequired = numWanted - numFound\n",
    "            \n",
    "            #fit model:\n",
    "            cutoff = np.percentile(scores[isTrain],0.3)\n",
    "            model.fit(buildTrain(indptr, isTrain, 1), scores[isTrain]<cutoff)\n",
    "\n",
    "            #predict (slowest step):\n",
    "            proba = chunkPredictProba(model, indptr, isTrain, 1)\n",
    "        \n",
    "            #rank the probabilities\n",
    "            proba_sorted = (-proba).argsort()\n",
    "            \n",
    "            #sorted the unseen instances by probability (highest prob first):\n",
    "            test = test[proba_sorted]\n",
    "\n",
    "            #topK molecules already found in the training set:\n",
    "            numSampled = np.argmax(np.cumsum(topK[test])>numRequired)\n",
    "            \n",
    "            #df.loc[count] = ['morgan_feat', size, percent, numWanted, numSampled+size]\n",
    "            df.loc[count] = ['morgan_feat', size,numWanted, numSampled+size]\n",
    "            count+=1\n",
    "            print(count, size, numWanted, numSampled+size)\n",
    "                \n",
    "            df.to_csv('../processed_data/AmpC_single_'+str(0.3)+'.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
