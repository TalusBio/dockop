{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import tqdm\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('default')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.renderers.enable('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_CHUNKS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some indices\n",
    "Even the sparse matrices won't fit in memory. So we will have to loop through them when making predictions or sampling random items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_EMBEDDINGS = True\n",
    "TOP_K_THRESHOLD = 50_000\n",
    "MODEL = \"_r1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECEPTOR = \"AmpC\"\n",
    "DATA_DIR = \"/mnt/efs/AmpC_data\"\n",
    "\n",
    "MODEL_PATH = \"/mnt/efs/mol2vec/examples/models/model_300dim.pkl\"\n",
    "UNCOMMON = \"UNK\"\n",
    "\n",
    "if USE_EMBEDDINGS:\n",
    "    OUTPUT_RESULTS_FILE = f\"{RECEPTOR}_embedding_results{MODEL}.csv\"\n",
    "else:\n",
    "    OUTPUT_RESULTS_FILE = f\"{RECEPTOR}_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of items:\n",
    "indptr = [0]\n",
    "scores_lst = []\n",
    "\n",
    "# if USE_EMBEDDINGS:\n",
    "    # scores = np.load(f\"{INPUT_DATA_DIR}/{RECEPTOR}_embedding_scores.npy\")\n",
    "    # vectors = np.load(f\"{INPUT_DATA_DIR}/{RECEPTOR}_embeddings.npy\")\n",
    "# else:\n",
    "    # scores = np.load(f\"{INPUT_DATA_DIR}/{RECEPTOR}_scores.npy\")\n",
    "    # vectors = sparse.load_npz(f\"{INPUT_DATA_DIR}/{RECEPTOR}_fingerprints.npz\")\n",
    "\n",
    "for chunk_id in range(NUM_CHUNKS):\n",
    "    scores = np.load(f\"{DATA_DIR}/{RECEPTOR}_scores_{chunk_id}{MODEL}.npy\")\n",
    "    indptr.append(indptr[-1] + scores.shape[0])\n",
    "    scores_lst.append(scores)\n",
    "    \n",
    "scores = np.concatenate(scores_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96214206,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=4)\n",
    "def load_vectors(chunk_id):\n",
    "    print(f\"Loading vectors {chunk_id}{MODEL}\", end=\"; \", flush=True)\n",
    "    if USE_EMBEDDINGS:\n",
    "        vectors = np.load(f\"{DATA_DIR}/{RECEPTOR}_embeddings_{chunk_id}{MODEL}.npy\")\n",
    "    else:\n",
    "        vectors = sparse.load_npz(f\"{DATA_DIR}/{RECEPTOR}_fingerprints_{chunk_id}.npz\")\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vectors(chunk_id, indptr, is_train):\n",
    "    print(f\"Extracting vectors: {chunk_id}\", end=\"; \", flush=True)\n",
    "    vectors = load_vectors(chunk_id)\n",
    "    mask = is_train[indptr[chunk_id]:indptr[chunk_id+1]]\n",
    "    return vectors[mask]\n",
    "\n",
    "def build_train(indptr, is_train):\n",
    "    print(\"Building training set\", end=\"; \", flush=True)\n",
    "    if USE_EMBEDDINGS:\n",
    "        vectors = np.vstack([extract_vectors(i, tuple(indptr), is_train) for i in range(NUM_CHUNKS)])\n",
    "    else:\n",
    "        vectors = sparse.vstack([extract_vectors(i, tuple(indptr), is_train) for i in range(NUM_CHUNKS)])  \n",
    "    return vectors\n",
    "\n",
    "def chunk_predict_proba(model, indptr, is_train):\n",
    "    print(\"Predicting proba\", end=\"; \", flush=True)\n",
    "    probas = []\n",
    "    for chunk_id in range(NUM_CHUNKS):\n",
    "        vectors = extract_vectors(chunk_id, indptr, ~is_train)\n",
    "        proba = model.predict_proba(vectors)[:,1]\n",
    "        probas.append(proba)\n",
    "    return np.concatenate(probas)\n",
    "\n",
    "def chunk_predict(model, indptr, is_train):\n",
    "    print(\"Predicting scores\", end=\"; \", flush=True)\n",
    "    preds = []\n",
    "    for chunk_id in range(NUM_CHUNKS):\n",
    "        vectors = extract_vectors(chunk_id, indptr, ~is_train)\n",
    "        pred = -1*model.predict(vectors) # best scoring will now be on top (like the proba)\n",
    "        preds.append(pred)\n",
    "    return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Logistic Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=10000, C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = (scores.argsort().argsort() < TOP_K_THRESHOLD)\n",
    "total = top_k.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Found 209 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 2, Found 14489 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 3, Found 21212 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 4, Found 27890 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 5, Found 32196 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 6, Found 35222 top k ligands\n",
      "Iteration: 7, Found 196 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 8, Found 14161 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 9, Found 21597 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 10, Found 28221 top k ligands\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 11, Found 32477 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 12, Found 35251 top k ligands\n",
      "Iteration: 13, Found 200 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 14, Found 15041 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 15, Found 22065 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 16, Found 28431 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 17, Found 32300 top k ligands\n",
      "Building training set; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; Fitting model; Predicting proba; Extracting vectors: 0; Loading vectors 0_r1; Extracting vectors: 1; Loading vectors 1_r1; Extracting vectors: 2; Loading vectors 2_r1; Extracting vectors: 3; Loading vectors 3_r1; Extracting vectors: 4; Loading vectors 4_r1; Extracting vectors: 5; Loading vectors 5_r1; Extracting vectors: 6; Loading vectors 6_r1; Extracting vectors: 7; Loading vectors 7_r1; Extracting vectors: 8; Loading vectors 8_r1; Extracting vectors: 9; Loading vectors 9_r1; \n",
      "Iteration: 18, Found 35131 top k ligands\n"
     ]
    }
   ],
   "source": [
    "training_set_fractions = [0.004, 0.002, 0.001]\n",
    "training_set_sizes = [400_000]\n",
    "\n",
    "percentile = 0.3\n",
    "\n",
    "df = pd.DataFrame(columns=['Algorithm', 'Training size', 'N ligands explored', '% top-k found'])\n",
    "count = 0\n",
    "\n",
    "for i in range(3):\n",
    "    idx = np.arange(scores.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    for size in training_set_sizes:\n",
    "#     for fraction in training_set_fractions:\n",
    "#         size = int(len(scores) * fraction)\n",
    "        \n",
    "        # split indices into train and test:\n",
    "        train_indices = idx[:size].copy()\n",
    "        test_indices = idx[size:].copy()\n",
    "        train_indices.sort()\n",
    "        test_indices.sort()\n",
    "\n",
    "        # generate a 'is a training instance' mask. \n",
    "        is_train = np.zeros(scores.shape[0]).astype(bool)\n",
    "        is_train[train_indices] = True\n",
    "\n",
    "        # top_k molecules already found in the training set:\n",
    "        num_found = top_k[train_indices].sum()\n",
    "\n",
    "        df.loc[count] = [\"morgan_feat\", size, train_indices.shape[0], num_found/total]\n",
    "        count += 1\n",
    "        print(f\"Iteration: {count}, Found {num_found} top k ligands\")\n",
    "\n",
    "        # estimate the cutoff once, from the initial random sample:\n",
    "        cutoff = np.percentile(scores[train_indices], percentile)\n",
    "\n",
    "        for i in range(5):\n",
    "            # fit logreg model:\n",
    "            x_train = build_train(indptr, is_train)\n",
    "            y_train = scores[is_train] < cutoff\n",
    "\n",
    "            print(\"Fitting model\", end=\"; \", flush=True)\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            # predict (slowest step) for logreg:\n",
    "            proba = chunk_predict_proba(model, indptr, is_train)\n",
    "\n",
    "            # rank the probabilities\n",
    "            proba_sorted = (-proba).argsort()\n",
    "\n",
    "            # rank the unseen instances:\n",
    "            test_indices = test_indices[proba_sorted]\n",
    "\n",
    "            # now append the next N instances from the rank ordered unseen instances onto the training set:\n",
    "            train_indices = np.concatenate([train_indices, test_indices[:size]])\n",
    "\n",
    "            # update the isTrain mask and remove those training instances from the test set\n",
    "            is_train[train_indices] = True\n",
    "            test_indices = test_indices[size:]\n",
    "\n",
    "            # keep the train and test idx arrays sorted so they agree with the chunked* methods:\n",
    "            test_indices.sort()\n",
    "            train_indices.sort()\n",
    "\n",
    "            # topK molecules already found in the training set:\n",
    "            num_found = top_k[train_indices].sum()\n",
    "\n",
    "            df.loc[count] = ['morgan_feat', size, train_indices.shape[0], num_found/total]\n",
    "            count += 1\n",
    "            \n",
    "            print(f\"\\nIteration: {count}, Found {num_found} top k ligands\")\n",
    "            \n",
    "            df.to_csv(f\"{DATA_DIR}/{OUTPUT_RESULTS_FILE}\")\n",
    "\n",
    "df.to_csv(f\"{DATA_DIR}/{OUTPUT_RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results look like this:\n",
    "And they can be plotted using `./plot_scripts/plot_wholedataset.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('../processed_data/ampc_reconstruction_0.3_1_.csv', index_col=0)\n",
    "df0['Algorithm'] = 'AmpC:LogReg (lewis)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(f\"{DATA_DIR}/{RECEPTOR}_embedding_results.csv\", index_col=0)\n",
    "df1['Algorithm'] = 'AmpC:LogReg (embeddings)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(f\"{DATA_DIR}/{RECEPTOR}_results.csv\", index_col=0)\n",
    "df2['Algorithm'] = 'AmpC:LogReg (fps)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df0, df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_results = [['AmpC:RF (Graff)', 400_000, 71.4, 2.1], \n",
    "                ['AmpC:NN (Graff)', 400_000, 74.7, 1.4],\n",
    "                ['AmpC:MPN (Graff)',400_000, 87.9, 2.3],\n",
    "                ['AmpC:RF (Graff)', 200_000, 45.5, 1.8],\n",
    "                ['AmpC:NN (Graff)', 200_000, 52.8, 0.5],\n",
    "                ['AmpC:MPN (Graff)', 200_000, 67.1, 2.1],\n",
    "                ['AmpC:RF (Graff)', 100_000, 24.0, 2.2],\n",
    "                ['AmpC:NN (Graff)', 100_000 , 33.3,0.3],\n",
    "                ['AmpC:MPN (Graff)', 100_000, 52.0, 0.5]]\n",
    "\n",
    "coley = pd.DataFrame(columns=['Algorithm', 'Training size', 'N ligands explored', '% top-k found'])\n",
    "count = 0 \n",
    "for res in prev_results:\n",
    "    desired_std_dev = res[3]\n",
    "    samples = np.array([-1,0,1]).astype(float)\n",
    "    samples *= (desired_std_dev/np.std(samples))\n",
    "    for s in samples:\n",
    "        coley.loc[count] = [res[0], res[1], res[1]*6, (s+res[2])/100]\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = pd.concat([df, coley])\n",
    "concat['% top-k found'] *= 100\n",
    "concat.columns = ['Algorithm', 'Training set size', 'N ligands explored', '% top-k found']\n",
    "concat['Training set size'] = concat['Training set size'].apply(lambda num: f\"{num:,d}\",)\n",
    "concat['Computation days (single CPU)'] = concat['N ligands explored'] / 60 / 60 /24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_bars = alt.Chart(concat).mark_errorbar(extent='ci').encode(\n",
    "  x=alt.X('N ligands explored:Q',title='Number of ligands sampled'),\n",
    "  y=alt.Y('% top-k found:Q', title='% top 50,000 found'),\n",
    "    color=alt.Color('Algorithm')\n",
    ")\n",
    "\n",
    "points = alt.Chart(concat).mark_point(filled=False, size=40, color='black').encode(\n",
    "  x=alt.X('N ligands explored:Q'),\n",
    "  y=alt.Y('% top-k found:Q',aggregate='mean',title='% top 50,000 found'),\n",
    "    color=alt.Color('Algorithm'),\n",
    "    tooltip=alt.Tooltip('% top-k found:Q',aggregate='mean',title='% top 50,000 found')\n",
    ")\n",
    "\n",
    "line = alt.Chart(concat).mark_line(color='black',size=2,opacity=0.5).encode(\n",
    "  x=alt.X('N ligands explored:Q'),\n",
    "  y=alt.Y('% top-k found:Q',aggregate='mean',title='% top 50,000 found'),\n",
    "    color=alt.Color('Algorithm')\n",
    ")\n",
    "\n",
    "ch = (error_bars+points+line).properties(height=300,width=150).facet(\n",
    "    column=alt.Column('Training set size:N',sort=alt.Sort([0.004, 0.002, 0.001])),\n",
    ").resolve_scale(x='independent')\n",
    "# ch.save('../../figures/active_learning_percentage.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dockop",
   "language": "python",
   "name": "dockop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
