{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some indices\n",
    "Even the sparse matrices won't fit in memory. So we will have to loop through them when making predictions or sampling random items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECEPTOR = \"FEN1\"\n",
    "OUTPUT_DATA_DIR = \"../processed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.load(f\"../processed_data/{RECEPTOR}_scores.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints = sparse.load_npz(f\"../processed_data/{RECEPTOR}_fingerprints.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((355771,), (355771, 8192))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape, fingerprints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369.0, 355402.0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.sum(), len(scores)-scores.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Logistic Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=10000, C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = scores\n",
    "total = scores.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Altair, using three repeats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Found 1.0 top k ligands\n",
      "Iteration: 2, Found 1.0 top k ligands\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 354486 is out of bounds for axis 0 with size 353771",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-9398c548b838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# rank the unseen instances:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mtest_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mproba_sorted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# now append the next N instances from the rank ordered unseen instances onto the training set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 354486 is out of bounds for axis 0 with size 353771"
     ]
    }
   ],
   "source": [
    "training_set_sizes = [1_000, 5_000, 10_000]\n",
    "\n",
    "percentile = 0.3\n",
    "\n",
    "df = pd.DataFrame(columns=['Algorithm', 'Training size', 'N ligands explored', '% top-k found'])\n",
    "count = 0\n",
    "\n",
    "for i in range(3):\n",
    "    idx = np.arange(scores.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    for size in training_set_sizes:\n",
    "        # split indices into train and test:\n",
    "        train_indices = idx[:size].copy()\n",
    "        test_indices = idx[size:].copy()\n",
    "        train_indices.sort()\n",
    "        test_indices.sort()\n",
    "\n",
    "        # generate a 'is a training instance' mask. \n",
    "        is_train = np.zeros(scores.shape[0]).astype(bool)\n",
    "        is_train[train_indices] = True\n",
    "\n",
    "        # top_k molecules already found in the training set:\n",
    "        num_found = top_k[train_indices].sum()\n",
    "\n",
    "        df.loc[count] = [\"morgan_feat\", size, train_indices.shape[0], num_found/total]\n",
    "        count += 1\n",
    "        print(f\"Iteration: {count}, Found {num_found} top k ligands\")\n",
    "\n",
    "        # estimate the cutoff once, from the initial random sample:\n",
    "        # cutoff = np.percentile(scores[train_indices], percentile)\n",
    "\n",
    "        for i in range(5):\n",
    "            # fit logreg model:\n",
    "            x_train = fingerprints[is_train]\n",
    "            y_train = scores[is_train]\n",
    "            x_val = fingerprints[~is_train]\n",
    "            y_val = scores[~is_train]\n",
    "            \n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            # predict (slowest step) for logreg:\n",
    "            proba = model.predict_proba(x_val)[:, 1]\n",
    "\n",
    "            # rank the probabilities\n",
    "            proba_sorted = (-proba).argsort()\n",
    "\n",
    "            # rank the unseen instances:\n",
    "            test_indices = test_indices[proba_sorted]\n",
    "\n",
    "            # now append the next N instances from the rank ordered unseen instances onto the training set:\n",
    "            train = np.concatenate([train_indices, test_indices[:size]])\n",
    "\n",
    "            # update the isTrain mask and remove those training instances from the test set\n",
    "            is_train[train_indices] = True\n",
    "            test_indices = test_indices[size:]\n",
    "\n",
    "            # keep the train and test idx arrays sorted so they agree with the chunked* methods:\n",
    "            test_indices.sort()\n",
    "            train_indices.sort()\n",
    "\n",
    "            # topK molecules already found in the training set:\n",
    "            num_found = top_k[train_indices].sum()\n",
    "\n",
    "            df.loc[count] = ['morgan_feat', size, train_indices.shape[0], num_found/total]\n",
    "            count += 1\n",
    "            \n",
    "            print(f\"Iteration: {count}, Found {num_found} top k ligands\")\n",
    "            \n",
    "            df.to_csv(f\"{OUTPUT_DATA_DIR}/{RECEPTOR}_results.csv\")\n",
    "\n",
    "df.to_csv(f\"{OUTPUT_DATA_DIR}/{RECEPTOR}_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results look like this:\n",
    "And they can be plotted using `./plot_scripts/plot_wholedataset.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv('../processed_data/ampc_reconstruction_0.3_1_.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dockop",
   "language": "python",
   "name": "dockop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
