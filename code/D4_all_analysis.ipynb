{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate some indices\n",
    "Even the sparse matrices won't fit in memory. So we will have to loop through them when making predictions or sampling random items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of items:\n",
    "indptr = [0]\n",
    "\n",
    "for chunkID in range(12):\n",
    "    scores = np.load(f'../processed_data/D4_all{chunkID}.npy')\n",
    "    indptr.append(indptr[-1] + scores.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.concatenate([np.load(f'../processed_data/D4_all{i}.npy') for i in range(12)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions to handle the slabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFPs(chunkID, indptr, isTrain):\n",
    "    fp = sparse.load_npz(f'../processed_data/D4_all{chunkID}.npz')\n",
    "    mask = isTrain[indptr[chunkID]:indptr[chunkID+1]]\n",
    "    return fp[mask]\n",
    "\n",
    "def buildTrain(indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('building training matrix')\n",
    "    fps = sparse.vstack([extractFPs(i, indptr, isTrain) for i in range(12)])\n",
    "    return fps\n",
    "\n",
    "def chunkPredictProba(model, indptr, isTrain, verbose=0):\n",
    "    if verbose:\n",
    "        print('predicting probabilities')\n",
    "    probas = []\n",
    "    for chunkID in range(12):\n",
    "        fps = extractFPs(chunkID, indptr, ~isTrain)\n",
    "        proba = model.predict_proba(fps)[:,1]\n",
    "        probas.append(proba)\n",
    "    return np.concatenate(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and RF regressor and Logistic Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=10000, C=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = (scores.argsort().argsort() < (scores.shape[0]*0.0005)) #0.05th percentile.\n",
    "\n",
    "#topK = (scores.argsort().argsort() < 50_000) #~0.05th percentile.\n",
    "tot = topK.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58121"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Altair, using three repeats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 193\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "2 13693\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "3 23078\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "4 29606\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "5 33615\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "6 36732\n",
      "7 98\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "8 7436\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "9 15632\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "10 21821\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "11 25981\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "12 28940\n",
      "13 53\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "14 4401\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "15 9417\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "16 14506\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "17 18297\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "18 21194\n",
      "19 211\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "20 14270\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "21 22956\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "22 29890\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "23 34043\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "24 37114\n",
      "25 97\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "26 7736\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "27 14934\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "28 21263\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "29 25817\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "30 28913\n",
      "31 51\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "32 3339\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "33 8818\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "34 14012\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "35 18158\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "36 21131\n",
      "37 195\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "38 13440\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "39 22991\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "40 29893\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "41 34199\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "42 37268\n",
      "43 100\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "44 7032\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "45 14324\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "46 20872\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "47 25387\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "48 28950\n",
      "49 50\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "50 3400\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "51 8736\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "52 14016\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "53 18038\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "54 21192\n",
      "1 206\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "2 12569\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "3 20063\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "4 25919\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "5 30202\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "6 33654\n",
      "7 111\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "8 6632\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "9 13332\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "10 18285\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "11 21748\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "12 24837\n",
      "13 59\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "14 3329\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "15 8414\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "16 12532\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "17 15254\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "18 17760\n",
      "19 189\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "20 12648\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "21 20385\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "22 26281\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "23 30480\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "24 33814\n",
      "25 105\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "26 6056\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "27 12664\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "28 17781\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "29 21206\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "30 24289\n",
      "31 56\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "32 2721\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "33 7424\n",
      "building training matrix\n",
      "predicting probabilities\n",
      "34 11562\n",
      "building training matrix\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainingSetSizes=[400_000, 200_000, 100_000]\n",
    "#for percentile in [0.05, 0.1, 0.25, 0.5, 0.75, 1 ]:\n",
    "for percentile in [0.125, 1.5, 1.75]:\n",
    "    cutoff = np.percentile(scores, percentile)\n",
    "    df = pd.DataFrame(columns=['Algorithm', 'Training size', 'N ligands explored', '% top-k found'])\n",
    "    count=0\n",
    "    \n",
    "    for i in range(3):\n",
    "        idx = np.arange(scores.shape[0])\n",
    "        np.random.shuffle(idx)\n",
    "\n",
    "        for size in trainingSetSizes:\n",
    "            #split indices into train and test:\n",
    "            train = idx[:size].copy()\n",
    "            test = idx[size:].copy()\n",
    "            train.sort()\n",
    "            test.sort()\n",
    "    \n",
    "            #generate a 'is a training instance' mask. \n",
    "            isTrain = np.zeros(scores.shape[0]).astype(bool)\n",
    "            isTrain[train]=True\n",
    "    \n",
    "            #topK molecules already found in the training set:\n",
    "            numFound = topK[train].sum()\n",
    "        \n",
    "            df.loc[count] = ['morgan_feat', size, train.shape[0], numFound/tot]\n",
    "            count+=1\n",
    "            print(count, numFound)\n",
    "    \n",
    "            for i in range(5):\n",
    "                #fit model:\n",
    "                model.fit(buildTrain(indptr, isTrain, 1), scores[isTrain]<cutoff)\n",
    "    \n",
    "                #predict (slowest step):\n",
    "                proba = chunkPredictProba(model, indptr, isTrain, 1)\n",
    "    \n",
    "                #rank the probabilities\n",
    "                proba_sorted = (-proba).argsort()\n",
    "        \n",
    "                #rank the unseen instances:\n",
    "                test = test[proba_sorted]\n",
    "\n",
    "                #now append the next N instances from the rank ordered unseen instances onto the training set:\n",
    "                train = np.concatenate([train, test[:size]])\n",
    "        \n",
    "                #update the isTrain mask:\n",
    "                isTrain[train]=True\n",
    "        \n",
    "                #now remove those training instances from the test set:\n",
    "                test = test[size:]\n",
    "\n",
    "                #keep the train and test idx arrays sorted so they agree with the chunked* methods:\n",
    "                test.sort()\n",
    "                train.sort()\n",
    "        \n",
    "                #topK molecules already found in the training set:\n",
    "                numFound = topK[train].sum()\n",
    "            \n",
    "                df.loc[count] = ['morgan_feat', size, train.shape[0], numFound/tot]\n",
    "                count+=1\n",
    "                print(count, numFound)\n",
    "                \n",
    "    df.to_csv('../processed_data/D4_reconstruction_'+str(percentile)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../processed_data/ampc_reconstruction_0.1.csv')\n",
    "df = pd.read_csv('../processed_data/ampc_reconstruction_0.125.csv')\n",
    "#df = pd.read_csv('../processed_data/ampc_reconstruction_0.15.csv')\n",
    "#df = pd.read_csv('../processed_data/ampc_reconstruction_0.175.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Training size</th>\n",
       "      <th>N ligands explored</th>\n",
       "      <th>% top-k found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.00424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>800000</td>\n",
       "      <td>0.35218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0.65792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>1600000</td>\n",
       "      <td>0.79070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>2000000</td>\n",
       "      <td>0.86268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>2400000</td>\n",
       "      <td>0.90034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>0.00218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.18236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>600000</td>\n",
       "      <td>0.41914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>800000</td>\n",
       "      <td>0.57312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0.67264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0.74560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.00146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>200000</td>\n",
       "      <td>0.09404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>300000</td>\n",
       "      <td>0.25992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.36042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.42898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>600000</td>\n",
       "      <td>0.48248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.00422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>800000</td>\n",
       "      <td>0.36392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0.66630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>1600000</td>\n",
       "      <td>0.78924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>2000000</td>\n",
       "      <td>0.85260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>2400000</td>\n",
       "      <td>0.88914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>0.00220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.18922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>600000</td>\n",
       "      <td>0.43240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>800000</td>\n",
       "      <td>0.56910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0.66302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0.73272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.00118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>200000</td>\n",
       "      <td>0.09392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>300000</td>\n",
       "      <td>0.25790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.36564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.43476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>600000</td>\n",
       "      <td>0.48818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.00388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>800000</td>\n",
       "      <td>0.35664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0.67168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>1600000</td>\n",
       "      <td>0.80010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>2000000</td>\n",
       "      <td>0.86598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>400000</td>\n",
       "      <td>2400000</td>\n",
       "      <td>0.90612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>200000</td>\n",
       "      <td>0.00230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.18568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>600000</td>\n",
       "      <td>0.41492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>800000</td>\n",
       "      <td>0.55818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0.64616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>200000</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0.71348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.00114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>200000</td>\n",
       "      <td>0.09180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>300000</td>\n",
       "      <td>0.25500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>400000</td>\n",
       "      <td>0.36546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.43476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>morgan_feat</td>\n",
       "      <td>100000</td>\n",
       "      <td>600000</td>\n",
       "      <td>0.48788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0    Algorithm  Training size  N ligands explored  % top-k found\n",
       "0            0  morgan_feat         400000              400000        0.00424\n",
       "1            1  morgan_feat         400000              800000        0.35218\n",
       "2            2  morgan_feat         400000             1200000        0.65792\n",
       "3            3  morgan_feat         400000             1600000        0.79070\n",
       "4            4  morgan_feat         400000             2000000        0.86268\n",
       "5            5  morgan_feat         400000             2400000        0.90034\n",
       "6            6  morgan_feat         200000              200000        0.00218\n",
       "7            7  morgan_feat         200000              400000        0.18236\n",
       "8            8  morgan_feat         200000              600000        0.41914\n",
       "9            9  morgan_feat         200000              800000        0.57312\n",
       "10          10  morgan_feat         200000             1000000        0.67264\n",
       "11          11  morgan_feat         200000             1200000        0.74560\n",
       "12          12  morgan_feat         100000              100000        0.00146\n",
       "13          13  morgan_feat         100000              200000        0.09404\n",
       "14          14  morgan_feat         100000              300000        0.25992\n",
       "15          15  morgan_feat         100000              400000        0.36042\n",
       "16          16  morgan_feat         100000              500000        0.42898\n",
       "17          17  morgan_feat         100000              600000        0.48248\n",
       "18          18  morgan_feat         400000              400000        0.00422\n",
       "19          19  morgan_feat         400000              800000        0.36392\n",
       "20          20  morgan_feat         400000             1200000        0.66630\n",
       "21          21  morgan_feat         400000             1600000        0.78924\n",
       "22          22  morgan_feat         400000             2000000        0.85260\n",
       "23          23  morgan_feat         400000             2400000        0.88914\n",
       "24          24  morgan_feat         200000              200000        0.00220\n",
       "25          25  morgan_feat         200000              400000        0.18922\n",
       "26          26  morgan_feat         200000              600000        0.43240\n",
       "27          27  morgan_feat         200000              800000        0.56910\n",
       "28          28  morgan_feat         200000             1000000        0.66302\n",
       "29          29  morgan_feat         200000             1200000        0.73272\n",
       "30          30  morgan_feat         100000              100000        0.00118\n",
       "31          31  morgan_feat         100000              200000        0.09392\n",
       "32          32  morgan_feat         100000              300000        0.25790\n",
       "33          33  morgan_feat         100000              400000        0.36564\n",
       "34          34  morgan_feat         100000              500000        0.43476\n",
       "35          35  morgan_feat         100000              600000        0.48818\n",
       "36          36  morgan_feat         400000              400000        0.00388\n",
       "37          37  morgan_feat         400000              800000        0.35664\n",
       "38          38  morgan_feat         400000             1200000        0.67168\n",
       "39          39  morgan_feat         400000             1600000        0.80010\n",
       "40          40  morgan_feat         400000             2000000        0.86598\n",
       "41          41  morgan_feat         400000             2400000        0.90612\n",
       "42          42  morgan_feat         200000              200000        0.00230\n",
       "43          43  morgan_feat         200000              400000        0.18568\n",
       "44          44  morgan_feat         200000              600000        0.41492\n",
       "45          45  morgan_feat         200000              800000        0.55818\n",
       "46          46  morgan_feat         200000             1000000        0.64616\n",
       "47          47  morgan_feat         200000             1200000        0.71348\n",
       "48          48  morgan_feat         100000              100000        0.00114\n",
       "49          49  morgan_feat         100000              200000        0.09180\n",
       "50          50  morgan_feat         100000              300000        0.25500\n",
       "51          51  morgan_feat         100000              400000        0.36546\n",
       "52          52  morgan_feat         100000              500000        0.43476\n",
       "53          53  morgan_feat         100000              600000        0.48788"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_results = [['RF (Coley)', 400_000, 71.4, 2.1], ['NN (Coley)', 400_000, 74.7, 1.4],\n",
    "                ['MPN (Coley)',400_000, 87.9, 2.3],\n",
    "    ['RF (Coley)', 200_000, 45.5, 1.8],\n",
    "['NN (Coley)', 200_000, 52.8, 0.5],\n",
    "['MPN (Coley)', 200_000, 67.1, 2.1],\n",
    "['RF (Coley)', 100_000, 24.0, 2.2],\n",
    "['NN (Coley)', 100_000 , 33.3,0.3],\n",
    "['MPN (Coley)', 100_000, 52.0, 0.5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "coley = pd.DataFrame(columns=['Algorithm', 'Training size', 'N ligands explored', '% top-k found'])\n",
    "count = 0 \n",
    "for res in prev_results:\n",
    "    \n",
    "    desired_std_dev = res[3]\n",
    "    samples = np.array([-1,0,1]).astype(float)\n",
    "    samples *= (desired_std_dev/np.std(samples))\n",
    "    for s in samples:\n",
    "        coley.loc[count]= [res[0], res[1], res[1]*6, (s+res[2])/100]\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = pd.concat([df, coley])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-6575f826544a4e8f801384dc785e0d07\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-6575f826544a4e8f801384dc785e0d07\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-6575f826544a4e8f801384dc785e0d07\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0221230900dd175db65e74669f0d1037\"}, \"facet\": {\"column\": {\"type\": \"nominal\", \"field\": \"Training size\", \"sort\": [0.004, 0.002, 0.001]}}, \"spec\": {\"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"ci\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\", \"title\": \"Number of ligands sampled\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"% top-k found\", \"title\": \"% top 50,000 found\"}}}, {\"mark\": {\"type\": \"point\", \"color\": \"black\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"title\": \"% top 50,000 found\"}}}, {\"mark\": {\"type\": \"line\", \"color\": \"black\", \"opacity\": 0.5, \"size\": 1}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"title\": \"% top 50,000 found\"}}}], \"height\": 400, \"width\": 200}, \"resolve\": {\"scale\": {\"x\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-0221230900dd175db65e74669f0d1037\": [{\"Unnamed: 0\": 0.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 400000, \"% top-k found\": 0.00424}, {\"Unnamed: 0\": 1.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 800000, \"% top-k found\": 0.35218}, {\"Unnamed: 0\": 2.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 1200000, \"% top-k found\": 0.65792}, {\"Unnamed: 0\": 3.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 1600000, \"% top-k found\": 0.7907}, {\"Unnamed: 0\": 4.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 2000000, \"% top-k found\": 0.8626799999999999}, {\"Unnamed: 0\": 5.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.9003399999999999}, {\"Unnamed: 0\": 6.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 200000, \"% top-k found\": 0.00218}, {\"Unnamed: 0\": 7.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 400000, \"% top-k found\": 0.18236}, {\"Unnamed: 0\": 8.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 600000, \"% top-k found\": 0.41913999999999996}, {\"Unnamed: 0\": 9.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 800000, \"% top-k found\": 0.57312}, {\"Unnamed: 0\": 10.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 1000000, \"% top-k found\": 0.67264}, {\"Unnamed: 0\": 11.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.7456}, {\"Unnamed: 0\": 12.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 100000, \"% top-k found\": 0.00146}, {\"Unnamed: 0\": 13.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 200000, \"% top-k found\": 0.09404}, {\"Unnamed: 0\": 14.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 300000, \"% top-k found\": 0.25992}, {\"Unnamed: 0\": 15.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 400000, \"% top-k found\": 0.36041999999999996}, {\"Unnamed: 0\": 16.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 500000, \"% top-k found\": 0.42898000000000003}, {\"Unnamed: 0\": 17.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.48248}, {\"Unnamed: 0\": 18.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 400000, \"% top-k found\": 0.004220000000000001}, {\"Unnamed: 0\": 19.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 800000, \"% top-k found\": 0.36391999999999997}, {\"Unnamed: 0\": 20.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 1200000, \"% top-k found\": 0.6663}, {\"Unnamed: 0\": 21.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 1600000, \"% top-k found\": 0.7892399999999999}, {\"Unnamed: 0\": 22.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 2000000, \"% top-k found\": 0.8526}, {\"Unnamed: 0\": 23.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.8891399999999999}, {\"Unnamed: 0\": 24.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 200000, \"% top-k found\": 0.0022}, {\"Unnamed: 0\": 25.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 400000, \"% top-k found\": 0.18922}, {\"Unnamed: 0\": 26.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 600000, \"% top-k found\": 0.4324}, {\"Unnamed: 0\": 27.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 800000, \"% top-k found\": 0.5691}, {\"Unnamed: 0\": 28.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 1000000, \"% top-k found\": 0.6630199999999999}, {\"Unnamed: 0\": 29.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.7327199999999999}, {\"Unnamed: 0\": 30.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 100000, \"% top-k found\": 0.00118}, {\"Unnamed: 0\": 31.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 200000, \"% top-k found\": 0.09392}, {\"Unnamed: 0\": 32.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 300000, \"% top-k found\": 0.2579}, {\"Unnamed: 0\": 33.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 400000, \"% top-k found\": 0.36564}, {\"Unnamed: 0\": 34.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 500000, \"% top-k found\": 0.43476000000000004}, {\"Unnamed: 0\": 35.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.48818}, {\"Unnamed: 0\": 36.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 400000, \"% top-k found\": 0.0038799999999999998}, {\"Unnamed: 0\": 37.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 800000, \"% top-k found\": 0.35664}, {\"Unnamed: 0\": 38.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 1200000, \"% top-k found\": 0.67168}, {\"Unnamed: 0\": 39.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 1600000, \"% top-k found\": 0.8001}, {\"Unnamed: 0\": 40.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 2000000, \"% top-k found\": 0.86598}, {\"Unnamed: 0\": 41.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.90612}, {\"Unnamed: 0\": 42.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 200000, \"% top-k found\": 0.0023}, {\"Unnamed: 0\": 43.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 400000, \"% top-k found\": 0.18567999999999998}, {\"Unnamed: 0\": 44.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 600000, \"% top-k found\": 0.41491999999999996}, {\"Unnamed: 0\": 45.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 800000, \"% top-k found\": 0.55818}, {\"Unnamed: 0\": 46.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 1000000, \"% top-k found\": 0.6461600000000001}, {\"Unnamed: 0\": 47.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.71348}, {\"Unnamed: 0\": 48.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 100000, \"% top-k found\": 0.00114}, {\"Unnamed: 0\": 49.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 200000, \"% top-k found\": 0.0918}, {\"Unnamed: 0\": 50.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 300000, \"% top-k found\": 0.255}, {\"Unnamed: 0\": 51.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 400000, \"% top-k found\": 0.36546}, {\"Unnamed: 0\": 52.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 500000, \"% top-k found\": 0.43476000000000004}, {\"Unnamed: 0\": 53.0, \"Algorithm\": \"morgan_feat\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.48788000000000004}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.6882803577007767}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7140000000000001}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7397196422992235}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7298535718005178}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.747}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7641464281994823}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.8508308679579935}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.879}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.9071691320420067}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.43295459231495137}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.455}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.4770454076850486}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.521876275643042}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.528}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.534123724356958}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.6452803577007765}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.6709999999999999}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.6967196422992233}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.21305561282938507}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.24}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.26694438717061497}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.32932576538582525}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.33299999999999996}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.3366742346141747}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.5138762756430421}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.52}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.526123724356958}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_bars = alt.Chart(concat).mark_errorbar(extent='ci').encode(\n",
    "  x=alt.X('N ligands explored:Q',title='Number of ligands sampled'),\n",
    "  y=alt.Y('% top-k found:Q', title='% top 50,000 found'),\n",
    "    color=alt.Color('Algorithm')\n",
    ")\n",
    "\n",
    "points = alt.Chart(concat).mark_point(filled=True, color='black').encode(\n",
    "  x=alt.X('N ligands explored:Q'),\n",
    "  y=alt.Y('% top-k found:Q',aggregate='mean',title='% top 50,000 found'),\n",
    "    color=alt.Color('Algorithm')\n",
    ")\n",
    "\n",
    "line = alt.Chart(concat).mark_line(color='black',size=1,opacity=0.5).encode(\n",
    "  x=alt.X('N ligands explored:Q'),\n",
    "  y=alt.Y('% top-k found:Q',aggregate='mean',title='% top 50,000 found'),\n",
    "    color=alt.Color('Algorithm')\n",
    ")\n",
    "\n",
    "ch = (error_bars+points+line).properties(height=400,width=200).facet(\n",
    "    column=alt.Column('Training size:N',sort=alt.Sort([0.004, 0.002, 0.001])),\n",
    ").resolve_scale(x='independent')\n",
    "ch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.to_csv('../processed_data/ampc_reconstruction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "chs = []\n",
    "for frac in [400000, 200000, 100000]:\n",
    "    \n",
    "    df_ = concat[concat['Training size']==frac].replace('morgan_feat', 'Morgan pharm. & Log.reg. (ours)')\n",
    "    error_bars = alt.Chart(df_).mark_errorbar(extent='ci').encode(\n",
    "          x=alt.X('N ligands explored:Q', \n",
    "                  title='Number of ligands sampled',\n",
    "                  scale=alt.Scale(domain=[0,max(df_['N ligands explored'])+10000])),\n",
    "          y=alt.Y('% top-k found:Q',scale=alt.Scale(domain=[0,0.95])),\n",
    "        color=alt.Color('Algorithm')\n",
    "        )\n",
    "\n",
    "    points = alt.Chart(df_).mark_point(filled=True, color='black').encode(\n",
    "          x=alt.X('N ligands explored:Q'),\n",
    "          y=alt.Y('% top-k found:Q',aggregate='mean',scale=alt.Scale(domain=[0,0.95])),\n",
    "        color=alt.Color('Algorithm')\n",
    "        )\n",
    "\n",
    "    line = alt.Chart(df_).mark_line(color='black',size=1,opacity=0.5).encode(\n",
    "          x=alt.X('N ligands explored:Q'),\n",
    "          y=alt.Y('% top-k found:Q',aggregate='mean',scale=alt.Scale(domain=[0,0.95])),\n",
    "        color=alt.Color('Algorithm')\n",
    "        )\n",
    "    ch = (error_bars+points+line).properties(width=200)\n",
    "    ch.title = str(frac / (100*1e6)*100)\n",
    "    chs.append(ch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "chs[0]\n",
    "sup = chs[0] |  chs[1] | chs[2]\n",
    "sup.save('../figures/ampC_reconstruction.html') #using 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-c06589d518d546808370e0ac42e315b8\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-c06589d518d546808370e0ac42e315b8\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-c06589d518d546808370e0ac42e315b8\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"ci\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\", \"scale\": {\"domain\": [0, 2410000]}, \"title\": \"Number of ligands sampled\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"mark\": {\"type\": \"point\", \"color\": \"black\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"mark\": {\"type\": \"line\", \"color\": \"black\", \"opacity\": 0.5, \"size\": 1}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"ci\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\", \"scale\": {\"domain\": [0, 1210000]}, \"title\": \"Number of ligands sampled\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"mark\": {\"type\": \"point\", \"color\": \"black\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"mark\": {\"type\": \"line\", \"color\": \"black\", \"opacity\": 0.5, \"size\": 1}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}], \"data\": {\"name\": \"data-7455ae1cdda928ca2d8cb4528a4807b2\"}, \"title\": \"0.2\", \"width\": 200}, {\"layer\": [{\"mark\": {\"type\": \"errorbar\", \"extent\": \"ci\"}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\", \"scale\": {\"domain\": [0, 610000]}, \"title\": \"Number of ligands sampled\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"mark\": {\"type\": \"point\", \"color\": \"black\", \"filled\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}, {\"mark\": {\"type\": \"line\", \"color\": \"black\", \"opacity\": 0.5, \"size\": 1}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Algorithm\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"N ligands explored\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"% top-k found\", \"scale\": {\"domain\": [0, 0.95]}}}}], \"data\": {\"name\": \"data-0c01a76e22be92105ddc77feeec60061\"}, \"title\": \"0.1\", \"width\": 200}], \"data\": {\"name\": \"data-088d808d9d564587e1089f10a2fd406a\"}, \"title\": \"0.4\", \"width\": 200, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-088d808d9d564587e1089f10a2fd406a\": [{\"Unnamed: 0\": 0.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 400000, \"% top-k found\": 0.0037}, {\"Unnamed: 0\": 1.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 800000, \"% top-k found\": 0.30772}, {\"Unnamed: 0\": 2.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 1200000, \"% top-k found\": 0.60858}, {\"Unnamed: 0\": 3.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 1600000, \"% top-k found\": 0.7477600000000001}, {\"Unnamed: 0\": 4.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 2000000, \"% top-k found\": 0.8266600000000001}, {\"Unnamed: 0\": 5.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.89136}, {\"Unnamed: 0\": 18.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 400000, \"% top-k found\": 0.00448}, {\"Unnamed: 0\": 19.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 800000, \"% top-k found\": 0.3252}, {\"Unnamed: 0\": 20.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 1200000, \"% top-k found\": 0.61252}, {\"Unnamed: 0\": 21.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 1600000, \"% top-k found\": 0.72344}, {\"Unnamed: 0\": 22.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 2000000, \"% top-k found\": 0.79804}, {\"Unnamed: 0\": 23.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.8627}, {\"Unnamed: 0\": 36.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 400000, \"% top-k found\": 0.00448}, {\"Unnamed: 0\": 37.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 800000, \"% top-k found\": 0.34968}, {\"Unnamed: 0\": 38.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 1200000, \"% top-k found\": 0.647}, {\"Unnamed: 0\": 39.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 1600000, \"% top-k found\": 0.77712}, {\"Unnamed: 0\": 40.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 2000000, \"% top-k found\": 0.84564}, {\"Unnamed: 0\": 41.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.8835799999999999}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.6882803577007767}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7140000000000001}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7397196422992235}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7298535718005178}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.747}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.7641464281994823}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.8508308679579935}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.879}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 400000, \"N ligands explored\": 2400000, \"% top-k found\": 0.9071691320420067}], \"data-7455ae1cdda928ca2d8cb4528a4807b2\": [{\"Unnamed: 0\": 6.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 200000, \"% top-k found\": 0.0017800000000000001}, {\"Unnamed: 0\": 7.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 400000, \"% top-k found\": 0.15298}, {\"Unnamed: 0\": 8.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 600000, \"% top-k found\": 0.38156}, {\"Unnamed: 0\": 9.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 800000, \"% top-k found\": 0.5052399999999999}, {\"Unnamed: 0\": 10.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 1000000, \"% top-k found\": 0.5795399999999999}, {\"Unnamed: 0\": 11.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.64248}, {\"Unnamed: 0\": 24.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 200000, \"% top-k found\": 0.0022}, {\"Unnamed: 0\": 25.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 400000, \"% top-k found\": 0.17504}, {\"Unnamed: 0\": 26.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 600000, \"% top-k found\": 0.41576}, {\"Unnamed: 0\": 27.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 800000, \"% top-k found\": 0.555}, {\"Unnamed: 0\": 28.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 1000000, \"% top-k found\": 0.63354}, {\"Unnamed: 0\": 29.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.69568}, {\"Unnamed: 0\": 42.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 200000, \"% top-k found\": 0.0025399999999999997}, {\"Unnamed: 0\": 43.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 400000, \"% top-k found\": 0.18758}, {\"Unnamed: 0\": 44.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 600000, \"% top-k found\": 0.42516000000000004}, {\"Unnamed: 0\": 45.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 800000, \"% top-k found\": 0.58024}, {\"Unnamed: 0\": 46.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 1000000, \"% top-k found\": 0.6659}, {\"Unnamed: 0\": 47.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.73056}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.43295459231495137}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.455}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.4770454076850486}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.521876275643042}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.528}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.534123724356958}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.6452803577007765}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.6709999999999999}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 200000, \"N ligands explored\": 1200000, \"% top-k found\": 0.6967196422992233}], \"data-0c01a76e22be92105ddc77feeec60061\": [{\"Unnamed: 0\": 12.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 100000, \"% top-k found\": 0.0009400000000000001}, {\"Unnamed: 0\": 13.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 200000, \"% top-k found\": 0.07122}, {\"Unnamed: 0\": 14.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 300000, \"% top-k found\": 0.22885999999999998}, {\"Unnamed: 0\": 15.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 400000, \"% top-k found\": 0.34031999999999996}, {\"Unnamed: 0\": 16.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 500000, \"% top-k found\": 0.41912}, {\"Unnamed: 0\": 17.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.48218}, {\"Unnamed: 0\": 30.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 100000, \"% top-k found\": 0.00116}, {\"Unnamed: 0\": 31.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 200000, \"% top-k found\": 0.08806}, {\"Unnamed: 0\": 32.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 300000, \"% top-k found\": 0.25188}, {\"Unnamed: 0\": 33.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 400000, \"% top-k found\": 0.35324}, {\"Unnamed: 0\": 34.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 500000, \"% top-k found\": 0.42628}, {\"Unnamed: 0\": 35.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.48294}, {\"Unnamed: 0\": 48.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 100000, \"% top-k found\": 0.00118}, {\"Unnamed: 0\": 49.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 200000, \"% top-k found\": 0.08394}, {\"Unnamed: 0\": 50.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 300000, \"% top-k found\": 0.24106}, {\"Unnamed: 0\": 51.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 400000, \"% top-k found\": 0.35702}, {\"Unnamed: 0\": 52.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 500000, \"% top-k found\": 0.44752}, {\"Unnamed: 0\": 53.0, \"Algorithm\": \"Morgan pharm. & Log.reg. (ours)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.53092}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.21305561282938507}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.24}, {\"Unnamed: 0\": null, \"Algorithm\": \"RF (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.26694438717061497}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.32932576538582525}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.33299999999999996}, {\"Unnamed: 0\": null, \"Algorithm\": \"NN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.3366742346141747}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.5138762756430421}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.52}, {\"Unnamed: 0\": null, \"Algorithm\": \"MPN (Coley)\", \"Training size\": 100000, \"N ligands explored\": 600000, \"% top-k found\": 0.526123724356958}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chs[0]+chs[1]+chs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup.save('../figures/ampC_reconstruction.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
